{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2028912-f1de-4907-82a3-a11848c07c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: lifelines in /opt/anaconda3/lib/python3.12/site-packages (0.30.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: autograd>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from lifelines) (1.8.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /opt/anaconda3/lib/python3.12/site-packages (from lifelines) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from lifelines) (1.2.1)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines) (1.14.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Install required packages and setup environment\n",
    "\"\"\"\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn statsmodels scipy openpyxl lifelines\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Dict, List\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from lifelines import WeibullAFTFitter, KaplanMeierFitter\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e304c862-2d62-4136-8307-831a7afaf883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core RBA-theta modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import required RBA-theta modules\n",
    "\"\"\"\n",
    "try:\n",
    "    import core.model as model\n",
    "    import core.helpers as helpers\n",
    "    from core.database import RBAThetaDB\n",
    "    import core.event_extraction as ee\n",
    "    print(\"Core RBA-theta modules imported successfully\")\n",
    "    CORE_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing core modules: {e}\")\n",
    "    print(\"Please ensure core modules are available\")\n",
    "    CORE_AVAILABLE = False\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f25c44-5ff1-4894-9738-76777f65edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Renamed 'Power' â†’ 'Turbine_1'\n",
      "Data loaded: (350640, 9)\n",
      "Turbines: 1\n",
      "Nominal value: 475.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and examine wind turbine dataset\n",
    "\"\"\"\n",
    "# Find data file\n",
    "data_paths = [\n",
    "    \"Baltic_Eagle.xlsx\",\n",
    "    \"./input_data/Baltic_Eagle.xlsx\",\n",
    "    \"data/Baltic_Eagle.xlsx\"\n",
    "]\n",
    "\n",
    "DATA_PATH = None\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        DATA_PATH = path\n",
    "        break\n",
    "\n",
    "if not DATA_PATH:\n",
    "    print(\"Data file not found. Please set DATA_PATH manually:\")\n",
    "    DATA_PATH = input(\"Enter path to new_8_wind_turbine_data.xlsx: \")\n",
    "\n",
    "# Load data\n",
    "original_data = pd.read_excel(DATA_PATH)\n",
    "time_col=\"time\"\n",
    "if time_col not in original_data.columns:\n",
    "        raise KeyError(f\"'{time_col}' column not found. Got: {list(original_data.columns)}\")\n",
    "\n",
    "    # Robust datetime parsing\n",
    "if not np.issubdtype(original_data[time_col].dtype, np.datetime64):\n",
    "    s = original_data[time_col].astype(str).str.strip()\n",
    "    parsed = None\n",
    "    try:\n",
    "        parsed = pd.to_datetime(s, format=\"ISO8601\", errors=\"raise\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    if parsed is None:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(s, format=\"%Y-%m-%d %H:%M:%S\", errors=\"raise\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if parsed is None:\n",
    "        parsed = pd.to_datetime(s, format=\"mixed\", errors=\"coerce\")\n",
    "\n",
    "    if parsed.isna().any():\n",
    "        bad_idx = parsed[parsed.isna()].index[:5].tolist()\n",
    "        raise ValueError(f\"Failed to parse some timestamps (examples idx: {bad_idx}).\")\n",
    "    original_data[time_col] = parsed\n",
    "\n",
    "original_data = original_data.set_index(time_col).sort_index()\n",
    "\n",
    "# Get turbine columns and nominal value\n",
    "original_data = original_data.rename(columns={\"Power\": \"Turbine_1\"})\n",
    "turbine_columns = [\"Turbine_1\"]\n",
    "print(\"[info] Renamed 'Power' â†’ 'Turbine_1'\")\n",
    "nominal = original_data[turbine_columns].max().max()\n",
    "\n",
    "print(f\"Data loaded: {original_data.shape}\")\n",
    "print(f\"Turbines: {len(turbine_columns)}\")\n",
    "print(f\"Nominal value: {nominal:.3f}\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"survival_analysis_results_Baltic_Data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3c37d1-2fc7-4ef1-9f70-052a383e8f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:core.model:Analyzing wind data for balanced ramp detection parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Extracting events from ENTIRE dataset using RBA-theta Traditional...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:core.model:Dataset analysis for method-specific detection:\n",
      "INFO:core.model:  - Data volatility: 0.733\n",
      "INFO:core.model:  - Meaningful change magnitude: 31.4404\n",
      "INFO:core.model:  - Substantial ramp frequency: 0.054\n",
      "INFO:core.model:  - Median substantial ramp duration: 12.0\n",
      "INFO:core.model:Adjusted mcmc_min_slope for balanced detection\n",
      "INFO:core.model:Adjusted trad_min_slope for balanced detection\n",
      "INFO:core.model:FIXED: stat_event_factor set to 0.000024 (much lower)\n",
      "INFO:core.model:FIXED: stationary lengths set to 7 (shorter)\n",
      "INFO:core.model:Method-specific adaptive parameters calculated:\n",
      "INFO:core.model:  - Traditional sig factor: 0.000080\n",
      "INFO:core.model:  - Traditional stat factor: 0.000024\n",
      "INFO:core.model:  - RF-MCMC sig factor: 0.000068\n",
      "INFO:core.model:  - RF-MCMC stat factor: 0.000024\n",
      "INFO:core.model:  - Traditional slope: 0.0500\n",
      "INFO:core.model:  - RF-MCMC slope: 0.0300\n",
      "INFO:core.model:  - Expected event rate: 6.0%\n",
      "INFO:core.database:Loaded 350640 data points for 1 turbines\n",
      "INFO:core.database:Normalized data with nominal value: 475.0\n",
      "INFO:core.database:Found turbine IDs: ['Turbine_1']\n",
      "INFO:core.event_extraction:Stationary detection - Input threshold: 0.000014\n",
      "INFO:core.event_extraction:Using data-driven threshold: 0.056166 (input was too small)\n",
      "INFO:core.event_extraction:Using refined window size: 12.0 for 350640 data points\n",
      "INFO:core.event_extraction:Pre-calculating rolling statistics...\n",
      "INFO:core.event_extraction:Refined thresholds: std=0.168499, range=0.336998, slope=0.028083\n",
      "INFO:core.event_extraction:Found 209388 potentially stationary points (after anti-ramp check)\n",
      "INFO:core.event_extraction:Refined stationary detection found 5907 events\n",
      "INFO:core.event_extraction:Stationary detection found 5907 raw events\n",
      "INFO:core.event_extraction:Filtering 5907 stationary events, data_std=0.3744\n",
      "INFO:core.event_extraction:Duration filter (min=4): 5876 events pass\n",
      "INFO:core.event_extraction:Stability filter (max_sigma=0.1498): 5825 events pass\n",
      "INFO:core.event_extraction:Quality filter: 5907 events pass\n",
      "INFO:core.event_extraction:Final result: 5796 stationary events pass all filters\n",
      "INFO:core.event_extraction:After filtering: 5796 stationary events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Events extracted from ENTIRE dataset:\n",
      "Total events: 12974\n",
      "Event types:\n",
      "event_type\n",
      "significant    7178\n",
      "stationary     5796\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Extract ALL events from complete dataset using RBA-theta Traditional\n",
    "This creates our complete ground truth before any splitting\n",
    "\"\"\"\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: Extracting events from ENTIRE dataset using RBA-theta Traditional...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get RBA-theta parameters\n",
    "param_config = model.tune_mixed_strategy(original_data, nominal)\n",
    "\n",
    "# Extract events using database approach (Traditional only)\n",
    "with RBAThetaDB(\":memory:\") as db:\n",
    "    db.load_data(original_data)\n",
    "    db.normalize_data(nominal)\n",
    "    turbine_ids = db.get_all_turbine_ids()\n",
    "    \n",
    "    all_sig_events_dict = {}\n",
    "    all_stat_events_dict = {}\n",
    "    \n",
    "    for turbine_id in turbine_ids:\n",
    "        turbine_data = db.get_turbine_data(turbine_id)\n",
    "        data_values = turbine_data['normalized_value'].values\n",
    "        \n",
    "        # Traditional method parameters only\n",
    "        adaptive_threshold = model.calculate_adaptive_threshold(data_values)\n",
    "        trad_sig_factor = param_config.get(\"trad_sig_event_factor\", 0.00003)\n",
    "        trad_stat_factor = param_config.get(\"trad_stat_event_factor\", 0.00009)\n",
    "        \n",
    "        all_sig_events_dict[turbine_id] = ee.significant_events(\n",
    "            data=data_values,\n",
    "            threshold=adaptive_threshold * trad_sig_factor,\n",
    "            min_duration=param_config.get(\"trad_min_duration\", 3),\n",
    "            min_slope=param_config.get(\"trad_min_slope\", 0.05),\n",
    "            window_minutes=param_config.get(\"trad_window\", 60),\n",
    "            freq_secs=param_config.get(\"trad_freq_secs\", 100),\n",
    "        )\n",
    "        \n",
    "        all_stat_events_dict[turbine_id] = ee.stationary_events(\n",
    "            data=data_values,\n",
    "            threshold=adaptive_threshold * trad_stat_factor,\n",
    "            min_duration=param_config.get(\"trad_min_duration\", 3),\n",
    "            min_stationary_length=param_config.get(\"trad_min_stationary_length\", 7),\n",
    "            window_minutes=param_config.get(\"trad_window\", 60),\n",
    "            freq_secs=param_config.get(\"trad_freq_secs\", 100),\n",
    "        )\n",
    "\n",
    "# Convert to combined DataFrame\n",
    "def convert_events_to_dataframe(sig_dict, stat_dict):\n",
    "    all_events = []\n",
    "    \n",
    "    for turbine_id in sig_dict.keys():\n",
    "        # Process significant events\n",
    "        sig_events = sig_dict[turbine_id]\n",
    "        if not sig_events.empty:\n",
    "            sig_copy = sig_events.copy()\n",
    "            sig_copy['event_type'] = 'significant'\n",
    "            turbine_num = int(turbine_id.split('_')[-1]) if '_' in str(turbine_id) else int(str(turbine_id).replace('Turbine_', ''))\n",
    "            sig_copy['turbine_id'] = turbine_num\n",
    "            all_events.append(sig_copy)\n",
    "        \n",
    "        # Process stationary events\n",
    "        stat_events = stat_dict[turbine_id]\n",
    "        if not stat_events.empty:\n",
    "            stat_copy = stat_events.copy()\n",
    "            stat_copy['event_type'] = 'stationary'\n",
    "            turbine_num = int(turbine_id.split('_')[-1]) if '_' in str(turbine_id) else int(str(turbine_id).replace('Turbine_', ''))\n",
    "            stat_copy['turbine_id'] = turbine_num\n",
    "            all_events.append(stat_copy)\n",
    "    \n",
    "    return pd.concat(all_events, ignore_index=True) if all_events else pd.DataFrame()\n",
    "\n",
    "# Get all events from entire dataset\n",
    "complete_events_data = convert_events_to_dataframe(all_sig_events_dict, all_stat_events_dict)\n",
    "\n",
    "print(f\"\\nEvents extracted from ENTIRE dataset:\")\n",
    "print(f\"Total events: {len(complete_events_data)}\")\n",
    "if not complete_events_data.empty:\n",
    "    print(\"Event types:\")\n",
    "    print(complete_events_data['event_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec97d9f-3ce7-4890-bfdb-99559a0836ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: Splitting raw data and events together...\n",
      "================================================================================\n",
      "Split indices - Train: 0:245447, Val: 245447:298044, Test: 298044:350640\n",
      "\n",
      "Data split completed:\n",
      "  Raw data - Train: (245447, 9), Val: (52597, 9), Test: (52596, 9)\n",
      "  Events - Train: 8957, Val: 2029, Test: 1988\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Split BOTH raw data and events data maintaining alignment\n",
    "70% train, 15% validation, 15% test\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Splitting raw data and events together...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate split indices\n",
    "n = len(original_data)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "print(f\"Split indices - Train: 0:{train_end}, Val: {train_end}:{val_end}, Test: {val_end}:{n}\")\n",
    "\n",
    "# Split raw data\n",
    "train_raw = original_data.iloc[:train_end].copy()\n",
    "val_raw = original_data.iloc[train_end:val_end].copy()\n",
    "test_raw = original_data.iloc[val_end:].copy()\n",
    "\n",
    "# Split events by time periods and adjust indices\n",
    "def filter_events_by_period(events_df, start_idx, end_idx):\n",
    "    if events_df.empty:\n",
    "        return events_df.copy()\n",
    "    \n",
    "    # Filter events within time period\n",
    "    period_events = events_df[\n",
    "        (events_df['t1'] >= start_idx) & \n",
    "        (events_df['t2'] < end_idx)\n",
    "    ].copy()\n",
    "    \n",
    "    # Adjust indices to be relative to period start\n",
    "    if not period_events.empty:\n",
    "        period_events['t1'] = period_events['t1'] - start_idx\n",
    "        period_events['t2'] = period_events['t2'] - start_idx\n",
    "    \n",
    "    return period_events\n",
    "\n",
    "# Split events for each period\n",
    "train_events = filter_events_by_period(complete_events_data, 0, train_end)\n",
    "val_events = filter_events_by_period(complete_events_data, train_end, val_end)\n",
    "test_events = filter_events_by_period(complete_events_data, val_end, n)\n",
    "\n",
    "print(f\"\\nData split completed:\")\n",
    "print(f\"  Raw data - Train: {train_raw.shape}, Val: {val_raw.shape}, Test: {test_raw.shape}\")\n",
    "print(f\"  Events - Train: {len(train_events)}, Val: {len(val_events)}, Test: {len(test_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123111d6-bb23-43b7-a5e8-ea9daa74728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TIMING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Context manager and decorator for timing code blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Operation\"):\n",
    "        self.name = name\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.elapsed = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nâ±ï¸  {self.name} started...\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.end_time = time.time()\n",
    "        self.elapsed = self.end_time - self.start_time\n",
    "        print(f\"âœ“ {self.name} completed in {self._format_time(self.elapsed)}\")\n",
    "    \n",
    "    def _format_time(self, seconds):\n",
    "        \"\"\"Format seconds into human-readable string\"\"\"\n",
    "        if seconds < 1:\n",
    "            return f\"{seconds*1000:.0f}ms\"\n",
    "        elif seconds < 60:\n",
    "            return f\"{seconds:.2f}s\"\n",
    "        elif seconds < 3600:\n",
    "            minutes = seconds / 60\n",
    "            return f\"{minutes:.1f}min\"\n",
    "        else:\n",
    "            hours = seconds / 3600\n",
    "            return f\"{hours:.2f}h\"\n",
    "\n",
    "# Global timing tracker\n",
    "timing_results = {\n",
    "    'label_creation': {},\n",
    "    'model_training': {},\n",
    "    'prediction': {},\n",
    "    'evaluation': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77e5ba60-bdb7-482e-8e57-8ed0b5edd42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "â±ï¸  Feature Engineering - Training started...\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING - TRAINING\n",
      "======================================================================\n",
      " Target: 'Turbine_1'\n",
      " Processing RBA event features...\n",
      "  Created event features\n",
      " Processing original features...\n",
      " Engineering target features...\n",
      " Creating temporal features...\n",
      "âœ“ Final shape: (245447, 122)\n",
      "âœ“ Feature Engineering - Training completed in 2.63s\n",
      "\n",
      "â±ï¸  Feature Engineering - Validation started...\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING - VALIDATION\n",
      "======================================================================\n",
      " Target: 'Turbine_1'\n",
      " Processing RBA event features...\n",
      "  Created event features\n",
      " Processing original features...\n",
      " Engineering target features...\n",
      " Creating temporal features...\n",
      "âœ“ Final shape: (52597, 122)\n",
      "âœ“ Feature Engineering - Validation completed in 687ms\n",
      "\n",
      "â±ï¸  Feature Engineering - Test started...\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING - TEST\n",
      "======================================================================\n",
      " Target: 'Turbine_1'\n",
      " Processing RBA event features...\n",
      "  Created event features\n",
      " Processing original features...\n",
      " Engineering target features...\n",
      " Creating temporal features...\n",
      "âœ“ Final shape: (52596, 122)\n",
      "âœ“ Feature Engineering - Test completed in 643ms\n",
      "\n",
      " Feature engineering complete for all splits!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_survival_features(raw_data, events_data, original_data, period_name,\n",
    "                             target_col='Turbine_1', turbine_id=1, \n",
    "                             exclude_cols=None, focus_on_events=True):\n",
    "    \"\"\"Survival-optimized feature engineering (from previous artifact)\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=raw_data.index)\n",
    "    \n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['time', 'timestamp']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FEATURE ENGINEERING - {period_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Target\n",
    "    features[target_col] = raw_data[target_col]\n",
    "    print(f\" Target: '{target_col}'\")\n",
    "    \n",
    "    # RBA event features\n",
    "    if events_data is not None and not events_data.empty:\n",
    "        print(f\" Processing RBA event features...\")\n",
    "        \n",
    "        n_points = len(raw_data)\n",
    "        event_magnitude = np.zeros(n_points)\n",
    "        event_duration = np.zeros(n_points)\n",
    "        event_slope = np.zeros(n_points)\n",
    "        event_sigma = np.zeros(n_points)\n",
    "        event_type_sig = np.zeros(n_points)\n",
    "        event_type_stat = np.zeros(n_points)\n",
    "        time_since_last_event = np.zeros(n_points)\n",
    "        event_intensity = np.zeros(n_points)\n",
    "        event_risk_score = np.zeros(n_points)\n",
    "        \n",
    "        turbine_events = events_data[events_data['turbine_id'] == turbine_id].sort_values('t1')\n",
    "        last_event_end = -1\n",
    "        \n",
    "        for _, event in turbine_events.iterrows():\n",
    "            try:\n",
    "                start_idx = int(event['t1'])\n",
    "                end_idx = int(event['t2'])\n",
    "                \n",
    "                if not (0 <= start_idx < n_points and 0 <= end_idx < n_points):\n",
    "                    continue\n",
    "                \n",
    "                duration = end_idx - start_idx + 1\n",
    "                event_slice = slice(start_idx, end_idx + 1)\n",
    "                \n",
    "                if event['event_type'] == 'significant':\n",
    "                    magnitude = abs(event.get('âˆ†w_m', 0))\n",
    "                    slope = event.get('Î¸_m', 0)\n",
    "                    sigma = event.get('Ïƒ_m', 0)\n",
    "                    \n",
    "                    event_magnitude[event_slice] = magnitude\n",
    "                    event_slope[event_slice] = slope\n",
    "                    event_sigma[event_slice] = sigma\n",
    "                    event_type_sig[event_slice] = 1\n",
    "                    event_intensity[event_slice] = magnitude / max(duration, 1)\n",
    "                    event_risk_score[event_slice] = magnitude * np.log1p(duration)\n",
    "                    \n",
    "                elif event['event_type'] == 'stationary':\n",
    "                    sigma = event.get('Ïƒ_s', 0)\n",
    "                    event_sigma[event_slice] = sigma\n",
    "                    event_type_stat[event_slice] = 1\n",
    "                    event_intensity[event_slice] = sigma\n",
    "                    event_risk_score[event_slice] = sigma * 0.5\n",
    "                \n",
    "                event_duration[event_slice] = duration\n",
    "                \n",
    "                if last_event_end >= 0:\n",
    "                    gap = start_idx - last_event_end\n",
    "                    time_since_last_event[event_slice] = gap\n",
    "                \n",
    "                last_event_end = end_idx\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        features['event_magnitude'] = event_magnitude\n",
    "        features['event_duration'] = event_duration\n",
    "        features['event_slope'] = event_slope\n",
    "        features['event_sigma'] = event_sigma\n",
    "        features['is_significant_event'] = event_type_sig\n",
    "        features['is_stationary_event'] = event_type_stat\n",
    "        features['time_since_last_event'] = time_since_last_event\n",
    "        features['event_intensity'] = event_intensity\n",
    "        features['event_risk_score'] = event_risk_score\n",
    "        \n",
    "        print(f\"  Created event features\")\n",
    "    \n",
    "    # Original data features\n",
    "    if original_data is not None:\n",
    "        print(f\" Processing original features...\")\n",
    "        \n",
    "        available_cols = [col for col in original_data.columns \n",
    "                         if col not in exclude_cols and col != target_col]\n",
    "        \n",
    "        for col in available_cols:\n",
    "            try:\n",
    "                base_name = col.replace(' ', '_').replace('-', '_')\n",
    "                features[base_name] = original_data.loc[raw_data.index, col]\n",
    "                \n",
    "                # Lags\n",
    "                for lag in [1, 2, 3, 6]:\n",
    "                    if lag <= len(features) // 20:\n",
    "                        features[f'{base_name}_lag{lag}'] = features[base_name].shift(lag)\n",
    "                \n",
    "                features[f'{base_name}_diff1'] = features[base_name].diff(1)\n",
    "                features[f'{base_name}_pct_change'] = features[base_name].pct_change().replace([np.inf, -np.inf], 0)\n",
    "                \n",
    "                for window in [6, 12]:\n",
    "                    if window <= len(features) // 10:\n",
    "                        features[f'{base_name}_rolling_mean_{window}'] = features[base_name].rolling(window).mean()\n",
    "                        features[f'{base_name}_rolling_std_{window}'] = features[base_name].rolling(window).std()\n",
    "                \n",
    "                if 'wind' in col.lower() or 'speed' in col.lower():\n",
    "                    if (features[base_name] >= 0).all():\n",
    "                        features[f'{base_name}_squared'] = features[base_name] ** 2\n",
    "                        features[f'{base_name}_cubed'] = features[base_name] ** 3\n",
    "                \n",
    "                if 'direction' in col.lower():\n",
    "                    features[f'{base_name}_sin'] = np.sin(np.radians(features[base_name]))\n",
    "                    features[f'{base_name}_cos'] = np.cos(np.radians(features[base_name]))\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Target features\n",
    "    print(f\" Engineering target features...\")\n",
    "    for lag in [1, 2, 3, 6]:\n",
    "        if lag <= len(features) // 20:\n",
    "            features[f'{target_col}_lag{lag}'] = features[target_col].shift(lag)\n",
    "    \n",
    "    features[f'{target_col}_diff1'] = features[target_col].diff(1)\n",
    "    features[f'{target_col}_pct_change'] = features[target_col].pct_change().replace([np.inf, -np.inf], 0)\n",
    "    features[f'{target_col}_volatility'] = features[target_col].rolling(12).std() / (features[target_col].rolling(12).mean() + 1e-6)\n",
    "    \n",
    "    for window in [6, 12]:\n",
    "        if window <= len(features) // 10:\n",
    "            features[f'{target_col}_rolling_mean_{window}'] = features[target_col].rolling(window).mean()\n",
    "            features[f'{target_col}_rolling_std_{window}'] = features[target_col].rolling(window).std()\n",
    "    \n",
    "    # Time features\n",
    "    if isinstance(features.index, pd.DatetimeIndex):\n",
    "        print(f\" Creating temporal features...\")\n",
    "        features['hour'] = features.index.hour\n",
    "        features['day_of_week'] = features.index.dayofweek\n",
    "        features['is_weekend'] = (features.index.dayofweek >= 5).astype(int)\n",
    "        features['hour_sin'] = np.sin(2 * np.pi * features.index.hour / 24)\n",
    "        features['hour_cos'] = np.cos(2 * np.pi * features.index.hour / 24)\n",
    "    \n",
    "    # Fill missing\n",
    "    features = features.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    features = features.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(f\"âœ“ Final shape: {features.shape}\")\n",
    "    return features\n",
    "\n",
    "# Generate features\n",
    "with Timer(\"Feature Engineering - Training\"):\n",
    "    train_features = create_survival_features(\n",
    "        train_raw, train_events, original_data.loc[train_raw.index],\n",
    "        \"Training\", target_col='Turbine_1', turbine_id=1\n",
    "    )\n",
    "\n",
    "with Timer(\"Feature Engineering - Validation\"):\n",
    "    val_features = create_survival_features(\n",
    "        val_raw, val_events, original_data.loc[val_raw.index],\n",
    "        \"Validation\", target_col='Turbine_1', turbine_id=1\n",
    "    )\n",
    "\n",
    "with Timer(\"Feature Engineering - Test\"):\n",
    "    test_features = create_survival_features(\n",
    "        test_raw, test_events, original_data.loc[test_raw.index],\n",
    "        \"Test\", target_col='Turbine_1', turbine_id=1\n",
    "    )\n",
    "\n",
    "print(\"\\n Feature engineering complete for all splits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b82c38c8-b537-4c6f-b5a7-175882dd104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: PREPARE POINT-WISE EVENT LABELS\n",
    "# ============================================================================\n",
    "\n",
    "def create_event_labels(raw_data, events_data, turbine_id=1):\n",
    "    \"\"\"\n",
    "    Create point-wise labels: for each timestep, is there an event?\n",
    "    Returns DataFrame with columns: is_event, event_type, event_magnitude, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"\\nğŸ“‹ Creating point-wise event labels...\")\n",
    "    \n",
    "    n_points = len(raw_data)\n",
    "    \n",
    "    # Initialize labels\n",
    "    labels = pd.DataFrame({\n",
    "        'is_event': np.zeros(n_points, dtype=int),\n",
    "        'event_type_sig': np.zeros(n_points, dtype=int),\n",
    "        'event_type_stat': np.zeros(n_points, dtype=int),\n",
    "        'event_magnitude': np.zeros(n_points, dtype=float),\n",
    "        'event_duration': np.zeros(n_points, dtype=float),\n",
    "        'event_position': np.zeros(n_points, dtype=float),  # Position within event (0=start, 1=end)\n",
    "    }, index=raw_data.index)\n",
    "    \n",
    "    # Fill in event labels\n",
    "    turbine_events = events_data[events_data['turbine_id'] == turbine_id]\n",
    "    \n",
    "    for _, event in turbine_events.iterrows():\n",
    "        start_idx = int(event['t1'])\n",
    "        end_idx = int(event['t2'])\n",
    "        \n",
    "        if end_idx >= n_points:\n",
    "            continue\n",
    "        \n",
    "        duration = end_idx - start_idx + 1\n",
    "        event_slice = slice(start_idx, end_idx + 1)\n",
    "        \n",
    "        # Mark as event\n",
    "        labels.loc[labels.index[event_slice], 'is_event'] = 1\n",
    "        labels.loc[labels.index[event_slice], 'event_duration'] = duration\n",
    "        \n",
    "        # Event type\n",
    "        if event['event_type'] == 'significant':\n",
    "            labels.loc[labels.index[event_slice], 'event_type_sig'] = 1\n",
    "            magnitude = abs(event.get('âˆ†w_m', 0))\n",
    "        else:\n",
    "            labels.loc[labels.index[event_slice], 'event_type_stat'] = 1\n",
    "            magnitude = event.get('Ïƒ_s', 0)\n",
    "        \n",
    "        labels.loc[labels.index[event_slice], 'event_magnitude'] = magnitude\n",
    "        \n",
    "        # Position within event (0 to 1)\n",
    "        positions = np.linspace(0, 1, duration)\n",
    "        labels.loc[labels.index[event_slice], 'event_position'] = positions\n",
    "    \n",
    "    event_coverage = (labels['is_event'] == 1).sum() / len(labels) * 100\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Event coverage: {event_coverage:.2f}%\")\n",
    "    print(f\"  Event points: {(labels['is_event'] == 1).sum()}\")\n",
    "    print(f\"  Non-event points: {(labels['is_event'] == 0).sum()}\")\n",
    "    print(f\"  Time taken: {elapsed:.2f}s\")\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78b4aeb7-2283-4fed-bc13-35fd859ceb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Direct event prediction classes loaded (clean)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: EVENT PREDICTION MODEL (cleaned)\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class DirectEventPredictor:\n",
    "    \"\"\"\n",
    "    Predict events directly from raw features (correct approach).\n",
    "    Trains:\n",
    "      - event_detector: binary classifier for event vs non-event (per timestep)\n",
    "      - event_type_classifier: classifier over event points (significant vs stationary)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_set: str = 'balanced', use_cv: bool = True):\n",
    "        self.feature_set = feature_set\n",
    "        self.use_cv = use_cv\n",
    "        self.event_detector: Optional[RandomForestClassifier] = None\n",
    "        self.event_type_classifier: Optional[RandomForestClassifier] = None\n",
    "        self.feature_columns: List[str] = []\n",
    "        self.scaler = RobustScaler()\n",
    "        self.fitted = False\n",
    "        self.training_time = None\n",
    "        self.prediction_time = None\n",
    "\n",
    "    # ------------------------------ feature selection ------------------------------\n",
    "    def select_features(self, features_df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Select features for prediction based on self.feature_set.\"\"\"\n",
    "        exclude = {\n",
    "            'Turbine_1', 'is_event', 'event_type_sig', 'event_type_stat',\n",
    "            'event_magnitude', 'event_duration', 'event_position'\n",
    "        }\n",
    "\n",
    "        if self.feature_set == 'minimal':\n",
    "            selected = [\n",
    "                'Turbine_1_lag1', 'Turbine_1_lag2', 'Turbine_1_lag3',\n",
    "                'Turbine_1_diff1', 'Turbine_1_rolling_mean_6',\n",
    "                'Turbine_1_rolling_std_6'\n",
    "            ]\n",
    "\n",
    "        elif self.feature_set == 'balanced':\n",
    "            selected = [\n",
    "                # lagged target features\n",
    "                'Turbine_1_lag1', 'Turbine_1_lag2', 'Turbine_1_lag3',\n",
    "                'Turbine_1_diff1', 'Turbine_1_pct_change', 'Turbine_1_volatility',\n",
    "                'Turbine_1_rolling_mean_6', 'Turbine_1_rolling_std_6'\n",
    "            ]\n",
    "\n",
    "            # wind features (cap at ~15 to avoid explosion)\n",
    "            wind_picks = 0\n",
    "            for col in features_df.columns:\n",
    "                if ('Windspeed' in col) or ('Wind_Direction' in col):\n",
    "                    if col not in selected:\n",
    "                        selected.append(col)\n",
    "                        wind_picks += 1\n",
    "                        if wind_picks >= 15:\n",
    "                            break\n",
    "\n",
    "            # temporal features\n",
    "            for col in features_df.columns:\n",
    "                if any(tok in col for tok in ['hour', 'day', 'weekend']):\n",
    "                    if col not in selected:\n",
    "                        selected.append(col)\n",
    "\n",
    "        else:  # 'full'\n",
    "            selected = [\n",
    "                col for col in features_df.columns\n",
    "                if (col not in exclude) and ('event' not in col.lower())\n",
    "            ]\n",
    "\n",
    "        # keep only those actually present\n",
    "        return [c for c in selected if c in features_df.columns]\n",
    "\n",
    "    # ------------------------------------- fit -------------------------------------\n",
    "    def fit(self, train_features: pd.DataFrame, train_labels: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Train models to predict events from raw features.\n",
    "\n",
    "        Args:\n",
    "            train_features: DataFrame of engineered features (lags, rolling stats, etc.)\n",
    "            train_labels: DataFrame with columns 'is_event' and 'event_type_sig'\n",
    "        \"\"\"\n",
    "        total_start = time.time()\n",
    "        print(f\"\\nğŸ”§ Training Direct Event Predictor ({self.feature_set})...\")\n",
    "\n",
    "        # Select and scale features\n",
    "        self.feature_columns = self.select_features(train_features)\n",
    "        print(f\"  Selected {len(self.feature_columns)} features\")\n",
    "\n",
    "        prep_start = time.time()\n",
    "        X = train_features[self.feature_columns].fillna(0).values\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        y_event = train_labels['is_event'].astype(int).values  # 0/1\n",
    "        # 1 = significant, 0 = stationary (for event points)\n",
    "        y_type = (train_labels['event_type_sig'] == 1).astype(int).values\n",
    "\n",
    "        prep_time = time.time() - prep_start\n",
    "        print(f\"  Training samples: {len(X)}\")\n",
    "        print(f\"  Event ratio: {y_event.mean()*100:.2f}%\")\n",
    "        print(f\"  Data prep time: {prep_time:.2f}s\")\n",
    "\n",
    "        # -------------------- MODEL 1: Event Detection (binary) ---------------------\n",
    "        print(f\"\\n  Training event detector...\")\n",
    "        detector_start = time.time()\n",
    "\n",
    "        if self.use_cv:\n",
    "            param_grid = {\n",
    "                'n_estimators': [200, 300],\n",
    "                'max_depth': [15, 20],\n",
    "                'min_samples_split': [20, 50],\n",
    "                'min_samples_leaf': [10, 20]\n",
    "            }\n",
    "            rf_event = RandomForestClassifier(\n",
    "                class_weight='balanced', random_state=42, n_jobs=-1\n",
    "            )\n",
    "            grid = GridSearchCV(\n",
    "                rf_event, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=0\n",
    "            )\n",
    "            grid.fit(X_scaled, y_event)\n",
    "            self.event_detector = grid.best_estimator_\n",
    "            print(f\"  Best params: {grid.best_params_}\")\n",
    "            print(f\"  CV F1-score: {grid.best_score_:.3f}\")\n",
    "        else:\n",
    "            self.event_detector = RandomForestClassifier(\n",
    "                n_estimators=300, max_depth=20,\n",
    "                min_samples_split=20, min_samples_leaf=10,\n",
    "                class_weight='balanced', random_state=42, n_jobs=-1\n",
    "            )\n",
    "            self.event_detector.fit(X_scaled, y_event)\n",
    "\n",
    "        detector_time = time.time() - detector_start\n",
    "        train_acc = self.event_detector.score(X_scaled, y_event)\n",
    "        print(f\"  âœ“ Training accuracy: {train_acc:.3f}\")\n",
    "        print(f\"  â±ï¸  Event detector time: {detector_time:.2f}s\")\n",
    "\n",
    "        # --------- MODEL 2: Event Type (significant vs stationary) on events --------\n",
    "        print(f\"\\n  Training event type classifier...\")\n",
    "        classifier_start = time.time()\n",
    "\n",
    "        event_mask = (y_event == 1)\n",
    "        X_events = X_scaled[event_mask]\n",
    "        y_type_events = y_type[event_mask]\n",
    "\n",
    "        if len(X_events) > 100:\n",
    "            if self.use_cv:\n",
    "                param_grid_type = {\n",
    "                    'n_estimators': [150, 200],\n",
    "                    'max_depth': [10, 15],\n",
    "                }\n",
    "                rf_type = RandomForestClassifier(\n",
    "                    class_weight='balanced', random_state=42, n_jobs=-1\n",
    "                )\n",
    "                grid_type = GridSearchCV(\n",
    "                    rf_type, param_grid_type, cv=3, scoring='f1', n_jobs=-1, verbose=0\n",
    "                )\n",
    "                grid_type.fit(X_events, y_type_events)\n",
    "                self.event_type_classifier = grid_type.best_estimator_\n",
    "                print(f\"  Best params: {grid_type.best_params_}\")\n",
    "                print(f\"  CV F1-score: {grid_type.best_score_:.3f}\")\n",
    "            else:\n",
    "                self.event_type_classifier = RandomForestClassifier(\n",
    "                    n_estimators=200, max_depth=15,\n",
    "                    class_weight='balanced', random_state=42, n_jobs=-1\n",
    "                )\n",
    "                self.event_type_classifier.fit(X_events, y_type_events)\n",
    "\n",
    "            type_acc = self.event_type_classifier.score(X_events, y_type_events)\n",
    "            print(f\"  Type accuracy: {type_acc:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Too few event samples for type classification (n={len(X_events)})\")\n",
    "            self.event_type_classifier = None\n",
    "\n",
    "        classifier_time = time.time() - classifier_start\n",
    "        print(f\"  â±ï¸  Type classifier time: {classifier_time:.2f}s\")\n",
    "\n",
    "        # Feature importance (event detector)\n",
    "        importances = self.event_detector.feature_importances_\n",
    "        top_idx = np.argsort(importances)[-10:][::-1]\n",
    "        print(f\"\\n  Top 10 features for event detection:\")\n",
    "        for i, idx in enumerate(top_idx, 1):\n",
    "            print(f\"    {i:2d}. {self.feature_columns[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "        total_time = time.time() - total_start\n",
    "        print(f\"\\n  TOTAL TRAINING TIME: {total_time:.2f}s ({total_time/60:.1f} min)\")\n",
    "        self.fitted = True\n",
    "        self.training_time = total_time\n",
    "        return self\n",
    "\n",
    "    # ----------------------------------- predict -----------------------------------\n",
    "    def predict(\n",
    "        self,\n",
    "        test_features: pd.DataFrame,\n",
    "        apply_rba_postprocess: bool = True,\n",
    "        min_event_duration: int = 3,\n",
    "        min_gap: int = 2,\n",
    "        event_threshold: float = 0.5,\n",
    "        type_threshold: float = 0.5,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict events from raw features.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: [t1, t2, event_type, confidence, duration, turbine_id]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Model not fitted. Call .fit() first.\")\n",
    "\n",
    "        pred_start = time.time()\n",
    "        print(f\"\\n Predicting events...\")\n",
    "\n",
    "        # Prepare inputs\n",
    "        prep_start = time.time()\n",
    "        X = test_features[self.feature_columns].fillna(0).values\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        prep_time = time.time() - prep_start\n",
    "\n",
    "        # Event probabilities\n",
    "        detect_start = time.time()\n",
    "        event_probs = self.event_detector.predict_proba(X_scaled)[:, 1]\n",
    "        detect_time = time.time() - detect_start\n",
    "\n",
    "        # Type probabilities (if classifier exists). If not, use neutral 0.5.\n",
    "        classify_start = time.time()\n",
    "        if self.event_type_classifier is not None:\n",
    "            type_probs = self.event_type_classifier.predict_proba(X_scaled)[:, 1]\n",
    "        else:\n",
    "            type_probs = np.full(len(X_scaled), 0.5)\n",
    "        classify_time = time.time() - classify_start\n",
    "\n",
    "        print(f\"  Prep: {prep_time:.2f}s | Detect: {detect_time:.2f}s | Type: {classify_time:.2f}s\")\n",
    "        print(f\"  Event probability: mean={event_probs.mean():.3f}, max={event_probs.max():.3f}\")\n",
    "\n",
    "        # ---- Post-processing: turn pointwise probs into segments ----\n",
    "        postproc_start = time.time()\n",
    "        is_event = (event_probs > event_threshold).astype(np.int32)\n",
    "        print(f\"  Predicted event points: {is_event.sum()} ({is_event.mean()*100:.2f}%)\")\n",
    "\n",
    "        events = []\n",
    "        in_event = False\n",
    "        event_start = None\n",
    "        probs_buf = []\n",
    "        type_buf = []\n",
    "\n",
    "        for t, flag in enumerate(is_event):\n",
    "            if flag and not in_event:\n",
    "                # start new event\n",
    "                in_event = True\n",
    "                event_start = t\n",
    "                probs_buf = [event_probs[t]]\n",
    "                type_buf = [type_probs[t]]\n",
    "            elif flag and in_event:\n",
    "                # continue event\n",
    "                probs_buf.append(event_probs[t])\n",
    "                type_buf.append(type_probs[t])\n",
    "            elif (not flag) and in_event:\n",
    "                # close event\n",
    "                t_end = t - 1\n",
    "                dur = t_end - event_start + 1\n",
    "                if dur >= min_event_duration:\n",
    "                    avg_conf = float(np.mean(probs_buf))\n",
    "                    avg_type = float(np.mean(type_buf))\n",
    "                    e_type = 'significant' if avg_type > type_threshold else 'stationary'\n",
    "                    events.append({\n",
    "                        't1': int(event_start),\n",
    "                        't2': int(t_end),\n",
    "                        'event_type': e_type,\n",
    "                        'confidence': avg_conf,\n",
    "                        'duration': int(dur),\n",
    "                    })\n",
    "                # reset\n",
    "                in_event = False\n",
    "                event_start = None\n",
    "                probs_buf = []\n",
    "                type_buf = []\n",
    "\n",
    "        # handle trailing event\n",
    "        if in_event and event_start is not None:\n",
    "            t_end = len(is_event) - 1\n",
    "            dur = t_end - event_start + 1\n",
    "            if dur >= min_event_duration:\n",
    "                avg_conf = float(np.mean(probs_buf)) if probs_buf else 0.0\n",
    "                avg_type = float(np.mean(type_buf)) if type_buf else 0.5\n",
    "                e_type = 'significant' if avg_type > type_threshold else 'stationary'\n",
    "                events.append({\n",
    "                    't1': int(event_start),\n",
    "                    't2': int(t_end),\n",
    "                    'event_type': e_type,\n",
    "                    'confidence': avg_conf,\n",
    "                    'duration': int(dur),\n",
    "                })\n",
    "\n",
    "        events_df = pd.DataFrame(events)\n",
    "\n",
    "        # Minimum gap filtering\n",
    "        if apply_rba_postprocess and len(events_df) > 0 and min_gap > 0:\n",
    "            events_df = events_df.sort_values('t1').reset_index(drop=True)\n",
    "            filtered = []\n",
    "            last_end = -min_gap - 1\n",
    "            for _, row in events_df.iterrows():\n",
    "                if int(row['t1']) - last_end > min_gap:\n",
    "                    filtered.append(row.to_dict())\n",
    "                    last_end = int(row['t2'])\n",
    "            events_df = pd.DataFrame(filtered)\n",
    "\n",
    "        if len(events_df) > 0:\n",
    "            events_df['turbine_id'] = 1\n",
    "\n",
    "        postproc_time = time.time() - postproc_start\n",
    "        total_pred_time = time.time() - pred_start\n",
    "        self.prediction_time = total_pred_time\n",
    "\n",
    "        print(f\"  Post-processing time: {postproc_time:.2f}s\")\n",
    "        print(f\"  TOTAL PREDICTION TIME: {total_pred_time:.2f}s\")\n",
    "        print(f\"  âœ“ Extracted {len(events_df)} events\")\n",
    "        if len(events_df) > 0:\n",
    "            print(f\"    Significant: {(events_df['event_type']=='significant').sum()}\")\n",
    "            print(f\"    Stationary: {(events_df['event_type']=='stationary').sum()}\")\n",
    "\n",
    "        return events_df\n",
    "\n",
    "\n",
    "print(\"âœ“ Direct event prediction classes loaded (clean)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e455a2e-0c7c-4307-b2f4-d1a024ebf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_event_predictions(true_events, predicted_events, tolerance=3):\n",
    "    \"\"\"\n",
    "    Evaluate predicted events against ground truth\n",
    "    \n",
    "    Args:\n",
    "        true_events: DataFrame with ground truth events\n",
    "        predicted_events: DataFrame with predicted events\n",
    "        tolerance: Time steps tolerance for matching (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1-score\n",
    "    \"\"\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Track which true events have been matched\n",
    "    matched_true = set()\n",
    "    \n",
    "    # For each predicted event, try to find matching true event\n",
    "    for _, pred in predicted_events.iterrows():\n",
    "        pred_start, pred_end = pred['t1'], pred['t2']\n",
    "        pred_type = pred['event_type']\n",
    "        \n",
    "        matched = False\n",
    "        \n",
    "        for idx, true in true_events.iterrows():\n",
    "            if idx in matched_true:\n",
    "                continue\n",
    "            \n",
    "            true_start, true_end = true['t1'], true['t2']\n",
    "            true_type = true['event_type']\n",
    "            \n",
    "            # Check if events overlap or are within tolerance\n",
    "            start_overlap = abs(pred_start - true_start) <= tolerance\n",
    "            end_overlap = abs(pred_end - true_end) <= tolerance\n",
    "            type_match = pred_type == true_type\n",
    "            \n",
    "            # Consider it a match if timing is close and type matches\n",
    "            if (start_overlap or end_overlap) and type_match:\n",
    "                true_positives += 1\n",
    "                matched_true.add(idx)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            false_positives += 1\n",
    "    \n",
    "    # Count false negatives (true events not matched)\n",
    "    false_negatives = len(true_events) - len(matched_true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'total_predicted': len(predicted_events),\n",
    "        'total_true': len(true_events)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_by_event_type(true_events, predicted_events, tolerance=3):\n",
    "    \"\"\"Evaluate separately for significant and stationary events\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for event_type in ['significant', 'stationary']:\n",
    "        true_subset = true_events[true_events['event_type'] == event_type]\n",
    "        pred_subset = predicted_events[predicted_events['event_type'] == event_type]\n",
    "        \n",
    "        results[event_type] = evaluate_event_predictions(\n",
    "            true_subset, pred_subset, tolerance\n",
    "        )\n",
    "    \n",
    "    # Overall\n",
    "    results['overall'] = evaluate_event_predictions(\n",
    "        true_events, predicted_events, tolerance\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5033e33f-2f4a-4fc4-954b-fac3c1391a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING DIRECT EVENT PREDICTION MODELS\n",
      "================================================================================\n",
      "\n",
      "â±ï¸  Label Creation - Training started...\n",
      "\n",
      "ğŸ“‹ Creating point-wise event labels...\n",
      "  Event coverage: 73.37%\n",
      "  Event points: 180092\n",
      "  Non-event points: 65355\n",
      "  Time taken: 18.52s\n",
      "âœ“ Label Creation - Training completed in 18.52s\n",
      "\n",
      "â±ï¸  Label Creation - Validation started...\n",
      "\n",
      "ğŸ“‹ Creating point-wise event labels...\n",
      "  Event coverage: 75.50%\n",
      "  Event points: 39710\n",
      "  Non-event points: 12887\n",
      "  Time taken: 5.50s\n",
      "âœ“ Label Creation - Validation completed in 5.50s\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION: MINIMAL\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  Training - MINIMAL started...\n",
      "\n",
      "ğŸ”§ Training Direct Event Predictor (minimal)...\n",
      "  Selected 6 features\n",
      "  Training samples: 245447\n",
      "  Event ratio: 73.37%\n",
      "  Data prep time: 0.08s\n",
      "\n",
      "  Training event detector...\n",
      "  Best params: {'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 200}\n",
      "  CV F1-score: 0.770\n",
      "  âœ“ Training accuracy: 0.769\n",
      "  â±ï¸  Event detector time: 1182.61s\n",
      "\n",
      "  Training event type classifier...\n",
      "  Best params: {'max_depth': 15, 'n_estimators': 150}\n",
      "  CV F1-score: 0.799\n",
      "  Type accuracy: 0.908\n",
      "  â±ï¸  Type classifier time: 134.19s\n",
      "\n",
      "  Top 10 features for event detection:\n",
      "     1. Turbine_1_rolling_std_6: 0.2591\n",
      "     2. Turbine_1_rolling_mean_6: 0.2016\n",
      "     3. Turbine_1_lag3: 0.1413\n",
      "     4. Turbine_1_lag1: 0.1393\n",
      "     5. Turbine_1_lag2: 0.1318\n",
      "     6. Turbine_1_diff1: 0.1269\n",
      "\n",
      "  TOTAL TRAINING TIME: 1318.52s (22.0 min)\n",
      "âœ“ Training - MINIMAL completed in 22.0min\n",
      "\n",
      " VALIDATION:\n",
      "\n",
      "â±ï¸  Validation Prediction - MINIMAL started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.01s | Detect: 0.28s | Type: 0.16s\n",
      "  Event probability: mean=0.569, max=0.994\n",
      "  Predicted event points: 33329 (63.37%)\n",
      "  Post-processing time: 0.22s\n",
      "  TOTAL PREDICTION TIME: 0.67s\n",
      "  âœ“ Extracted 2228 events\n",
      "    Significant: 822\n",
      "    Stationary: 1406\n",
      "âœ“ Validation Prediction - MINIMAL completed in 672ms\n",
      "\n",
      "â±ï¸  Validation Evaluation - MINIMAL started...\n",
      "âœ“ Validation Evaluation - MINIMAL completed in 3.3min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.6023\n",
      "    Recall:    0.6614\n",
      "    F1-Score:  0.6305\n",
      "\n",
      "ğŸ“Š TEST:\n",
      "\n",
      "â±ï¸  Test Prediction - MINIMAL started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.02s | Detect: 0.40s | Type: 0.17s\n",
      "  Event probability: mean=0.567, max=0.993\n",
      "  Predicted event points: 33096 (62.92%)\n",
      "  Post-processing time: 0.29s\n",
      "  TOTAL PREDICTION TIME: 0.88s\n",
      "  âœ“ Extracted 2277 events\n",
      "    Significant: 861\n",
      "    Stationary: 1416\n",
      "âœ“ Test Prediction - MINIMAL completed in 883ms\n",
      "\n",
      "â±ï¸  Test Evaluation - MINIMAL started...\n",
      "âœ“ Test Evaluation - MINIMAL completed in 3.6min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.5894\n",
      "    Recall:    0.6751\n",
      "    F1-Score:  0.6293\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION: BALANCED\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  Training - BALANCED started...\n",
      "\n",
      "ğŸ”§ Training Direct Event Predictor (balanced)...\n",
      "  Selected 28 features\n",
      "  Training samples: 245447\n",
      "  Event ratio: 73.37%\n",
      "  Data prep time: 0.57s\n",
      "\n",
      "  Training event detector...\n",
      "  Best params: {'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 300}\n",
      "  CV F1-score: 0.867\n",
      "  âœ“ Training accuracy: 0.864\n",
      "  â±ï¸  Event detector time: 2710.78s\n",
      "\n",
      "  Training event type classifier...\n",
      "  Best params: {'max_depth': 15, 'n_estimators': 200}\n",
      "  CV F1-score: 0.904\n",
      "  Type accuracy: 0.974\n",
      "  â±ï¸  Type classifier time: 308.01s\n",
      "\n",
      "  Top 10 features for event detection:\n",
      "     1. Turbine_1_volatility: 0.1479\n",
      "     2. Windspeed_rolling_std_12: 0.1360\n",
      "     3. Windspeed_rolling_mean_12: 0.1255\n",
      "     4. Windspeed_lag6: 0.0702\n",
      "     5. Turbine_1_rolling_std_6: 0.0539\n",
      "     6. Windspeed_rolling_std_6: 0.0376\n",
      "     7. Turbine_1_rolling_mean_6: 0.0357\n",
      "     8. Windspeed_rolling_mean_6: 0.0353\n",
      "     9. Windspeed_lag3: 0.0249\n",
      "    10. Windspeed: 0.0228\n",
      "\n",
      "  TOTAL TRAINING TIME: 3022.04s (50.4 min)\n",
      "âœ“ Training - BALANCED completed in 50.4min\n",
      "\n",
      " VALIDATION:\n",
      "\n",
      "â±ï¸  Validation Prediction - BALANCED started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.10s | Detect: 0.56s | Type: 0.20s\n",
      "  Event probability: mean=0.625, max=0.999\n",
      "  Predicted event points: 36965 (70.28%)\n",
      "  Post-processing time: 0.22s\n",
      "  TOTAL PREDICTION TIME: 1.08s\n",
      "  âœ“ Extracted 1955 events\n",
      "    Significant: 757\n",
      "    Stationary: 1198\n",
      "âœ“ Validation Prediction - BALANCED completed in 1.09s\n",
      "\n",
      "â±ï¸  Validation Evaluation - BALANCED started...\n",
      "âœ“ Validation Evaluation - BALANCED completed in 2.7min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.7729\n",
      "    Recall:    0.7447\n",
      "    F1-Score:  0.7585\n",
      "\n",
      "ğŸ“Š TEST:\n",
      "\n",
      "â±ï¸  Test Prediction - BALANCED started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.06s | Detect: 0.77s | Type: 0.22s\n",
      "  Event probability: mean=0.617, max=0.998\n",
      "  Predicted event points: 36243 (68.91%)\n",
      "  Post-processing time: 0.21s\n",
      "  TOTAL PREDICTION TIME: 1.26s\n",
      "  âœ“ Extracted 1968 events\n",
      "    Significant: 804\n",
      "    Stationary: 1164\n",
      "âœ“ Test Prediction - BALANCED completed in 1.26s\n",
      "\n",
      "â±ï¸  Test Evaluation - BALANCED started...\n",
      "âœ“ Test Evaluation - BALANCED completed in 2.7min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.7612\n",
      "    Recall:    0.7535\n",
      "    F1-Score:  0.7573\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION: FULL\n",
      "======================================================================\n",
      "\n",
      "â±ï¸  Training - FULL started...\n",
      "\n",
      "ğŸ”§ Training Direct Event Predictor (full)...\n",
      "  Selected 112 features\n",
      "  Training samples: 245447\n",
      "  Event ratio: 73.37%\n",
      "  Data prep time: 1.93s\n",
      "\n",
      "  Training event detector...\n",
      "  Best params: {'max_depth': 20, 'min_samples_leaf': 10, 'min_samples_split': 20, 'n_estimators': 300}\n",
      "  CV F1-score: 0.872\n",
      "  âœ“ Training accuracy: 0.891\n",
      "  â±ï¸  Event detector time: 5922.05s\n",
      "\n",
      "  Training event type classifier...\n",
      "  Best params: {'max_depth': 15, 'n_estimators': 150}\n",
      "  CV F1-score: 0.909\n",
      "  Type accuracy: 0.980\n",
      "  â±ï¸  Type classifier time: 719.34s\n",
      "\n",
      "  Top 10 features for event detection:\n",
      "     1. Turbine_1_rolling_std_12: 0.1142\n",
      "     2. Power_of_V174_9.5_rolling_std_12: 0.1067\n",
      "     3. Turbine_1_volatility: 0.0376\n",
      "     4. Power_of_V174_9.5_rolling_mean_12: 0.0310\n",
      "     5. Turbine_1_rolling_mean_12: 0.0289\n",
      "     6. Scaled_Windspeed_(at_107m)_rolling_std_12: 0.0272\n",
      "     7. Windspeed_rolling_std_12: 0.0245\n",
      "     8. Power_of_V174_9.5_rolling_std_6: 0.0221\n",
      "     9. Turbine_1_rolling_std_6: 0.0216\n",
      "    10. Scaled_Windspeed_(at_107m)_rolling_mean_12: 0.0216\n",
      "\n",
      "  TOTAL TRAINING TIME: 6647.05s (110.8 min)\n",
      "âœ“ Training - FULL completed in 1.85h\n",
      "\n",
      " VALIDATION:\n",
      "\n",
      "â±ï¸  Validation Prediction - FULL started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.28s | Detect: 0.90s | Type: 0.21s\n",
      "  Event probability: mean=0.639, max=1.000\n",
      "  Predicted event points: 37647 (71.58%)\n",
      "  Post-processing time: 0.34s\n",
      "  TOTAL PREDICTION TIME: 1.73s\n",
      "  âœ“ Extracted 1900 events\n",
      "    Significant: 747\n",
      "    Stationary: 1153\n",
      "âœ“ Validation Prediction - FULL completed in 1.75s\n",
      "\n",
      "â±ï¸  Validation Evaluation - FULL started...\n",
      "âœ“ Validation Evaluation - FULL completed in 2.7min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.7753\n",
      "    Recall:    0.7260\n",
      "    F1-Score:  0.7498\n",
      "\n",
      "ğŸ“Š TEST:\n",
      "\n",
      "â±ï¸  Test Prediction - FULL started...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.22s | Detect: 0.69s | Type: 0.20s\n",
      "  Event probability: mean=0.630, max=1.000\n",
      "  Predicted event points: 36924 (70.20%)\n",
      "  Post-processing time: 0.20s\n",
      "  TOTAL PREDICTION TIME: 1.31s\n",
      "  âœ“ Extracted 1939 events\n",
      "    Significant: 790\n",
      "    Stationary: 1149\n",
      "âœ“ Test Prediction - FULL completed in 1.32s\n",
      "\n",
      "â±ï¸  Test Evaluation - FULL started...\n",
      "âœ“ Test Evaluation - FULL completed in 2.8min\n",
      "\n",
      "  Results:\n",
      "    Precision: 0.7576\n",
      "    Recall:    0.7389\n",
      "    F1-Score:  0.7482\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DIRECT EVENT PREDICTION MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall pipeline timer\n",
    "pipeline_start = time.time()\n",
    "\n",
    "# Create point-wise labels\n",
    "with Timer(\"Label Creation - Training\"):\n",
    "    train_labels = create_event_labels(train_raw, train_events, turbine_id=1)\n",
    "\n",
    "with Timer(\"Label Creation - Validation\"):\n",
    "    val_labels = create_event_labels(val_raw, val_events, turbine_id=1)\n",
    "\n",
    "# Train models\n",
    "results_direct = {}\n",
    "total_times = {}\n",
    "\n",
    "for config in ['minimal', 'balanced', 'full']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONFIGURATION: {config.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    config_start = time.time()\n",
    "    \n",
    "    # Initialize\n",
    "    predictor = DirectEventPredictor(feature_set=config, use_cv=True)\n",
    "    \n",
    "    # Train\n",
    "    with Timer(f\"Training - {config.upper()}\"):\n",
    "        predictor.fit(train_features, train_labels)\n",
    "    \n",
    "    if not predictor.fitted:\n",
    "        continue\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VALIDATION\n",
    "    # ========================================================================\n",
    "    print(f\"\\n VALIDATION:\")\n",
    "    \n",
    "    with Timer(f\"Validation Prediction - {config.upper()}\"):\n",
    "        val_predicted = predictor.predict(\n",
    "            val_features,\n",
    "            apply_rba_postprocess=True,\n",
    "            min_event_duration=3,\n",
    "            min_gap=2\n",
    "        )\n",
    "    \n",
    "    with Timer(f\"Validation Evaluation - {config.upper()}\"):\n",
    "        val_metrics = evaluate_by_event_type(val_events, val_predicted, tolerance=5)\n",
    "    \n",
    "    print(f\"\\n  Results:\")\n",
    "    print(f\"    Precision: {val_metrics['overall']['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {val_metrics['overall']['recall']:.4f}\")\n",
    "    print(f\"    F1-Score:  {val_metrics['overall']['f1_score']:.4f}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST\n",
    "    # ========================================================================\n",
    "    print(f\"\\nğŸ“Š TEST:\")\n",
    "    \n",
    "    with Timer(f\"Test Prediction - {config.upper()}\"):\n",
    "        test_predicted = predictor.predict(\n",
    "            test_features,\n",
    "            apply_rba_postprocess=True,\n",
    "            min_event_duration=3,\n",
    "            min_gap=2\n",
    "        )\n",
    "    \n",
    "    with Timer(f\"Test Evaluation - {config.upper()}\"):\n",
    "        test_metrics = evaluate_by_event_type(test_events, test_predicted, tolerance=5)\n",
    "    \n",
    "    print(f\"\\n  Results:\")\n",
    "    print(f\"    Precision: {test_metrics['overall']['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {test_metrics['overall']['recall']:.4f}\")\n",
    "    print(f\"    F1-Score:  {test_metrics['overall']['f1_score']:.4f}\")\n",
    "    \n",
    "    config_time = time.time() - config_start\n",
    "    \n",
    "    # Store\n",
    "    results_direct[config] = {\n",
    "        'validation': val_metrics,\n",
    "        'test': test_metrics,\n",
    "        'predictor': predictor,\n",
    "        'total_time': config_time,\n",
    "        'training_time': predictor.training_time if hasattr(predictor, 'training_time') else 0,\n",
    "        'prediction_time': predictor.prediction_time if hasattr(predictor, 'prediction_time') else 0\n",
    "    }\n",
    "    \n",
    "    total_times[config] = config_time\n",
    "\n",
    "pipeline_time = time.time() - pipeline_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66eb84db-4453-4b69-b5c8-32660b9ba70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE RESULTS WITH TIMING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PERFORMANCE & TIMING SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Config Test_F1 Test_Prec Test_Recall Train_Time Pred_Time Total_Time\n",
      " MINIMAL  0.6293    0.5894      0.6751    1318.5s      0.9s    1734.6s\n",
      "BALANCED  0.7573    0.7612      0.7535    3022.0s      1.3s    3348.3s\n",
      "    FULL  0.7482    0.7576      0.7389    6647.1s      1.3s    6982.4s\n",
      "\n",
      "================================================================================\n",
      "ğŸ† BEST CONFIGURATION: BALANCED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Performance:\n",
      "   Test F1-Score:  0.7573 (75.7%)\n",
      "   Test Precision: 0.7612\n",
      "   Test Recall:    0.7535\n",
      "\n",
      "â±ï¸  Timing:\n",
      "   Training:       3022.0s (50.4min)\n",
      "   Prediction:     1.3s\n",
      "   Total:          3348.3s (55.8min)\n",
      "\n",
      "ğŸš€ Throughput:\n",
      "   Samples processed:  52,596\n",
      "   Processing speed:   41748 samples/second\n",
      "   Time per sample:    0.02ms\n",
      "\n",
      "ğŸ“Š Comparison to Previous Work:\n",
      "   Previous (SARIMAX indirect): F1 = 91.47%\n",
      "   Previous (Survival 6 feat):  F1 = 41.29%\n",
      "   Current (Direct prediction): F1 = 75.7%\n",
      "   âœ… EXCELLENT - Beats all previous approaches!\n",
      "\n",
      "â±ï¸  TOTAL PIPELINE TIME: 12089.4s (201.5min)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY WITH TIMING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLETE RESULTS WITH TIMING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results_direct) > 0:\n",
    "    print(\"\\nğŸ“Š PERFORMANCE & TIMING SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for config, res in results_direct.items():\n",
    "        test_f1 = res['test']['overall']['f1_score']\n",
    "        train_time = res.get('training_time', 0)\n",
    "        pred_time = res.get('prediction_time', 0)\n",
    "        total_time = res.get('total_time', 0)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Config': config.upper(),\n",
    "            'Test_F1': f\"{test_f1:.4f}\",\n",
    "            'Test_Prec': f\"{res['test']['overall']['precision']:.4f}\",\n",
    "            'Test_Recall': f\"{res['test']['overall']['recall']:.4f}\",\n",
    "            'Train_Time': f\"{train_time:.1f}s\",\n",
    "            'Pred_Time': f\"{pred_time:.1f}s\",\n",
    "            'Total_Time': f\"{total_time:.1f}s\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\" + summary_df.to_string(index=False))\n",
    "    \n",
    "    # Best configuration\n",
    "    best = max(results_direct.items(), key=lambda x: x[1]['test']['overall']['f1_score'])\n",
    "    best_name, best_res = best\n",
    "    best_f1 = best_res['test']['overall']['f1_score']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ† BEST CONFIGURATION: {best_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nğŸ“ˆ Performance:\")\n",
    "    print(f\"   Test F1-Score:  {best_f1:.4f} ({best_f1*100:.1f}%)\")\n",
    "    print(f\"   Test Precision: {best_res['test']['overall']['precision']:.4f}\")\n",
    "    print(f\"   Test Recall:    {best_res['test']['overall']['recall']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Timing:\")\n",
    "    print(f\"   Training:       {best_res.get('training_time', 0):.1f}s ({best_res.get('training_time', 0)/60:.1f}min)\")\n",
    "    print(f\"   Prediction:     {best_res.get('prediction_time', 0):.1f}s\")\n",
    "    print(f\"   Total:          {best_res.get('total_time', 0):.1f}s ({best_res.get('total_time', 0)/60:.1f}min)\")\n",
    "    \n",
    "    # Throughput analysis\n",
    "    test_samples = len(test_features)\n",
    "    pred_time = best_res.get('prediction_time', 1)\n",
    "    throughput = test_samples / pred_time\n",
    "    \n",
    "    print(f\"\\nğŸš€ Throughput:\")\n",
    "    print(f\"   Samples processed:  {test_samples:,}\")\n",
    "    print(f\"   Processing speed:   {throughput:.0f} samples/second\")\n",
    "    print(f\"   Time per sample:    {pred_time/test_samples*1000:.2f}ms\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(f\"\\nğŸ“Š Comparison to Previous Work:\")\n",
    "    print(f\"   Previous (SARIMAX indirect): F1 = 91.47%\")\n",
    "    print(f\"   Previous (Survival 6 feat):  F1 = 41.29%\")\n",
    "    print(f\"   Current (Direct prediction): F1 = {best_f1*100:.1f}%\")\n",
    "    \n",
    "    if best_f1 > 0.45:\n",
    "        print(f\"   âœ… EXCELLENT - Beats all previous approaches!\")\n",
    "    elif best_f1 > 0.35:\n",
    "        print(f\"   âœ… GOOD - Competitive with survival approach\")\n",
    "    elif best_f1 > 0.25:\n",
    "        print(f\"   âš ï¸  MODERATE - Shows promise, needs refinement\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  LOW - Requires threshold/parameter tuning\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  TOTAL PIPELINE TIME: {pipeline_time:.1f}s ({pipeline_time/60:.1f}min)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  No models were successfully trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a0a18b-68a9-47fc-8431-9c716dd84d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Identifying best configuration...\n",
      "âœ“ Best configuration: BALANCED\n",
      "  Test F1-Score: 0.7573\n",
      "\n",
      "ğŸ’¾ Saving best model...\n",
      "âœ“ Model saved to: survival_analysis_results_Baltic_Data/best_model_balanced.pkl\n",
      "  Model size: 422.01 MB\n",
      "\n",
      "ğŸ—ï¸  Saving model architecture...\n",
      "âœ“ Architecture saved to: survival_analysis_results_Baltic_Data/model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "# ============================================================================\n",
    "# 1. IDENTIFY BEST CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Identifying best configuration...\")\n",
    "\n",
    "best_config = max(results_direct.items(), \n",
    "                  key=lambda x: x[1]['test']['overall']['f1_score'])\n",
    "best_name = best_config[0]\n",
    "best_results = best_config[1]\n",
    "\n",
    "print(f\"âœ“ Best configuration: {best_name.upper()}\")\n",
    "print(f\"  Test F1-Score: {best_results['test']['overall']['f1_score']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SAVE BEST MODEL (PICKLE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ’¾ Saving best model...\")\n",
    "\n",
    "model_filename = os.path.join(OUTPUT_DIR, f'best_model_{best_name}.pkl')\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_results['predictor'], f)\n",
    "\n",
    "print(f\"âœ“ Model saved to: {model_filename}\")\n",
    "print(f\"  Model size: {os.path.getsize(model_filename) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SAVE MODEL ARCHITECTURE & HYPERPARAMETERS (JSON)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ—ï¸  Saving model architecture...\")\n",
    "\n",
    "model_architecture = {\n",
    "    'model_type': 'DirectEventPredictor',\n",
    "    'framework': 'sklearn RandomForestClassifier',\n",
    "    'feature_set': best_name,\n",
    "    'components': {\n",
    "        'event_detector': {\n",
    "            'type': 'RandomForestClassifier',\n",
    "            'n_estimators': best_results['predictor'].event_detector.n_estimators,\n",
    "            'max_depth': best_results['predictor'].event_detector.max_depth,\n",
    "            'min_samples_split': best_results['predictor'].event_detector.min_samples_split,\n",
    "            'min_samples_leaf': best_results['predictor'].event_detector.min_samples_leaf,\n",
    "            'class_weight': 'balanced',\n",
    "            'n_features': len(best_results['predictor'].feature_columns)\n",
    "        },\n",
    "        'event_type_classifier': {\n",
    "            'type': 'RandomForestClassifier',\n",
    "            'n_estimators': best_results['predictor'].event_type_classifier.n_estimators if best_results['predictor'].event_type_classifier else None,\n",
    "            'max_depth': best_results['predictor'].event_type_classifier.max_depth if best_results['predictor'].event_type_classifier else None,\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "    },\n",
    "    'features': {\n",
    "        'count': len(best_results['predictor'].feature_columns),\n",
    "        'list': best_results['predictor'].feature_columns,\n",
    "        'importance': {\n",
    "            best_results['predictor'].feature_columns[i]: float(imp)\n",
    "            for i, imp in enumerate(best_results['predictor'].event_detector.feature_importances_)\n",
    "        }\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'scaler': 'RobustScaler',\n",
    "        'missing_value_strategy': 'fillna(0)'\n",
    "    },\n",
    "    'postprocessing': {\n",
    "        'event_threshold': 0.5,\n",
    "        'min_event_duration': 3,\n",
    "        'min_gap_between_events': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "arch_filename = os.path.join(OUTPUT_DIR, 'model_architecture.json')\n",
    "with open(arch_filename, 'w') as f:\n",
    "    json.dump(model_architecture, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Architecture saved to: {arch_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ea3270-0e34-4fb4-834e-9998db8303d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Saving performance metrics...\n",
      "âœ“ Complete results saved to: survival_analysis_results_Baltic_Data/complete_results.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# ============================================================================\n",
    "# 4. SAVE COMPLETE RESULTS (JSON)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“ˆ Saving performance metrics...\")\n",
    "\n",
    "complete_results = {\n",
    "    'metadata': {\n",
    "        'model_name': 'Direct Event Prediction - RBA + Random Forest',\n",
    "        'model_type': 'Point-wise Binary Classification + Event Type Classification',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'dataset': 'Baltic Eagle Wind Turbine',\n",
    "        'author': 'Your Name',\n",
    "        'description': 'Direct event prediction using Random Forest trained on RBA-extracted event labels'\n",
    "    },\n",
    "    \n",
    "    'dataset_info': {\n",
    "        'total_samples': len(train_raw) + len(val_raw) + len(test_raw),\n",
    "        'training_samples': len(train_raw),\n",
    "        'validation_samples': len(val_raw),\n",
    "        'test_samples': len(test_raw),\n",
    "        'target_variable': 'Turbine_1',\n",
    "        'nominal_power': float(nominal) if 'nominal' in globals() else None,\n",
    "        'date_range': {\n",
    "            'start': str(train_raw.index[0]),\n",
    "            'end': str(test_raw.index[-1])\n",
    "        },\n",
    "        'event_coverage': {\n",
    "            'training': f\"{(train_labels['is_event'] == 1).sum() / len(train_labels) * 100:.2f}%\",\n",
    "            'validation': f\"{(val_labels['is_event'] == 1).sum() / len(val_labels) * 100:.2f}%\"\n",
    "        },\n",
    "        'total_events': {\n",
    "            'training': len(train_events),\n",
    "            'validation': len(val_events),\n",
    "            'test': len(test_events)\n",
    "        },\n",
    "        'event_distribution': {\n",
    "            'training': {\n",
    "                'significant': int((train_events['event_type'] == 'significant').sum()),\n",
    "                'stationary': int((train_events['event_type'] == 'stationary').sum())\n",
    "            },\n",
    "            'validation': {\n",
    "                'significant': int((val_events['event_type'] == 'significant').sum()),\n",
    "                'stationary': int((val_events['event_type'] == 'stationary').sum())\n",
    "            },\n",
    "            'test': {\n",
    "                'significant': int((test_events['event_type'] == 'significant').sum()),\n",
    "                'stationary': int((test_events['event_type'] == 'stationary').sum())\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'all_configurations': {}\n",
    "}\n",
    "\n",
    "# Add results for all configurations\n",
    "for config, results in results_direct.items():\n",
    "    complete_results['all_configurations'][config] = {\n",
    "        'feature_count': len(results['predictor'].feature_columns),\n",
    "        'training_time_seconds': float(results.get('training_time', 0)),\n",
    "        'prediction_time_seconds': float(results.get('prediction_time', 0)),\n",
    "        'total_time_seconds': float(results.get('total_time', 0)),\n",
    "        \n",
    "        'validation_performance': {\n",
    "            'overall': {\n",
    "                'precision': float(results['validation']['overall']['precision']),\n",
    "                'recall': float(results['validation']['overall']['recall']),\n",
    "                'f1_score': float(results['validation']['overall']['f1_score']),\n",
    "                'true_positives': int(results['validation']['overall']['true_positives']),\n",
    "                'false_positives': int(results['validation']['overall']['false_positives']),\n",
    "                'false_negatives': int(results['validation']['overall']['false_negatives']),\n",
    "                'total_predicted': int(results['validation']['overall']['total_predicted']),\n",
    "                'total_true': int(results['validation']['overall']['total_true'])\n",
    "            },\n",
    "            'by_event_type': {\n",
    "                'significant': {\n",
    "                    'precision': float(results['validation']['significant']['precision']),\n",
    "                    'recall': float(results['validation']['significant']['recall']),\n",
    "                    'f1_score': float(results['validation']['significant']['f1_score']),\n",
    "                    'true_positives': int(results['validation']['significant']['true_positives']),\n",
    "                    'false_positives': int(results['validation']['significant']['false_positives']),\n",
    "                    'false_negatives': int(results['validation']['significant']['false_negatives'])\n",
    "                },\n",
    "                'stationary': {\n",
    "                    'precision': float(results['validation']['stationary']['precision']),\n",
    "                    'recall': float(results['validation']['stationary']['recall']),\n",
    "                    'f1_score': float(results['validation']['stationary']['f1_score']),\n",
    "                    'true_positives': int(results['validation']['stationary']['true_positives']),\n",
    "                    'false_positives': int(results['validation']['stationary']['false_positives']),\n",
    "                    'false_negatives': int(results['validation']['stationary']['false_negatives'])\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'test_performance': {\n",
    "            'overall': {\n",
    "                'precision': float(results['test']['overall']['precision']),\n",
    "                'recall': float(results['test']['overall']['recall']),\n",
    "                'f1_score': float(results['test']['overall']['f1_score']),\n",
    "                'true_positives': int(results['test']['overall']['true_positives']),\n",
    "                'false_positives': int(results['test']['overall']['false_positives']),\n",
    "                'false_negatives': int(results['test']['overall']['false_negatives']),\n",
    "                'total_predicted': int(results['test']['overall']['total_predicted']),\n",
    "                'total_true': int(results['test']['overall']['total_true'])\n",
    "            },\n",
    "            'by_event_type': {\n",
    "                'significant': {\n",
    "                    'precision': float(results['test']['significant']['precision']),\n",
    "                    'recall': float(results['test']['significant']['recall']),\n",
    "                    'f1_score': float(results['test']['significant']['f1_score']),\n",
    "                    'true_positives': int(results['test']['significant']['true_positives']),\n",
    "                    'false_positives': int(results['test']['significant']['false_positives']),\n",
    "                    'false_negatives': int(results['test']['significant']['false_negatives'])\n",
    "                },\n",
    "                'stationary': {\n",
    "                    'precision': float(results['test']['stationary']['precision']),\n",
    "                    'recall': float(results['test']['stationary']['recall']),\n",
    "                    'f1_score': float(results['test']['stationary']['f1_score']),\n",
    "                    'true_positives': int(results['test']['stationary']['true_positives']),\n",
    "                    'false_positives': int(results['test']['stationary']['false_positives']),\n",
    "                    'false_negatives': int(results['test']['stationary']['false_negatives'])\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Best configuration details\n",
    "complete_results['best_configuration'] = {\n",
    "    'name': best_name,\n",
    "    'feature_count': len(best_results['predictor'].feature_columns),\n",
    "    'features': best_results['predictor'].feature_columns,\n",
    "    'test_f1_score': float(best_results['test']['overall']['f1_score']),\n",
    "    'test_precision': float(best_results['test']['overall']['precision']),\n",
    "    'test_recall': float(best_results['test']['overall']['recall']),\n",
    "    'training_time_seconds': float(best_results.get('training_time', 0)),\n",
    "    'training_time_minutes': float(best_results.get('training_time', 0)) / 60,\n",
    "    'prediction_time_seconds': float(best_results.get('prediction_time', 0)),\n",
    "    'throughput_samples_per_second': len(test_features) / best_results.get('prediction_time', 1)\n",
    "}\n",
    "\n",
    "# Performance interpretation\n",
    "test_f1 = best_results['test']['overall']['f1_score']\n",
    "if test_f1 >= 0.75:\n",
    "    quality = \"Excellent\"\n",
    "elif test_f1 >= 0.60:\n",
    "    quality = \"Good\"\n",
    "elif test_f1 >= 0.45:\n",
    "    quality = \"Fair\"\n",
    "else:\n",
    "    quality = \"Needs Improvement\"\n",
    "\n",
    "complete_results['performance_interpretation'] = {\n",
    "    'overall_quality': quality,\n",
    "    'f1_score_category': f\"{test_f1:.2%}\",\n",
    "    'comparison_to_previous': {\n",
    "        'sarimax_indirect': {\n",
    "            'f1_score': '91.47%',\n",
    "            'method': 'Time series prediction â†’ RBA extraction',\n",
    "            'comparison': 'Lower (SARIMAX is easier task)'\n",
    "        },\n",
    "        'survival_6_features': {\n",
    "            'f1_score': '41.29%',\n",
    "            'method': 'Inter-event timing prediction',\n",
    "            'comparison': f\"{'Better' if test_f1 > 0.4129 else 'Worse'} by {abs(test_f1 - 0.4129) / 0.4129 * 100:.1f}%\"\n",
    "        }\n",
    "    },\n",
    "    'summary': f\"Achieves {test_f1:.2%} F1-score for direct event prediction with {len(best_results['predictor'].feature_columns)} features\"\n",
    "}\n",
    "\n",
    "# Key findings\n",
    "complete_results['key_findings'] = {\n",
    "    'event_prediction': f\"F1-score of {test_f1:.4f} ({test_f1*100:.1f}%) for direct event detection\",\n",
    "    'precision_quality': \"High\" if best_results['test']['overall']['precision'] > 0.70 else \"Moderate\",\n",
    "    'recall_quality': \"High\" if best_results['test']['overall']['recall'] > 0.70 else \"Moderate\",\n",
    "    'generalization': \"Good\" if abs(best_results['validation']['overall']['f1_score'] - test_f1) < 0.05 else \"Fair\",\n",
    "    'speed': f\"Processes {len(test_features) / best_results.get('prediction_time', 1):.0f} samples/second\",\n",
    "    'production_readiness': \"Ready for deployment\" if test_f1 > 0.70 else \"Needs improvement\"\n",
    "}\n",
    "\n",
    "# Save complete results\n",
    "results_filename = os.path.join(OUTPUT_DIR, 'complete_results.json')\n",
    "with open(results_filename, 'w') as f:\n",
    "    json.dump(complete_results, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Complete results saved to: {results_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e2d27cd-9859-441c-9379-ccac7ef17725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Saving predictions...\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.04s | Detect: 0.63s | Type: 0.23s\n",
      "  Event probability: mean=0.625, max=0.999\n",
      "  Predicted event points: 36965 (70.28%)\n",
      "  Post-processing time: 0.21s\n",
      "  TOTAL PREDICTION TIME: 1.11s\n",
      "  âœ“ Extracted 1955 events\n",
      "    Significant: 757\n",
      "    Stationary: 1198\n",
      "âœ“ Validation predictions: survival_analysis_results_Baltic_Data/validation_predicted_events.csv\n",
      "\n",
      " Predicting events...\n",
      "  Prep: 0.02s | Detect: 0.61s | Type: 0.21s\n",
      "  Event probability: mean=0.617, max=0.998\n",
      "  Predicted event points: 36243 (68.91%)\n",
      "  Post-processing time: 0.20s\n",
      "  TOTAL PREDICTION TIME: 1.04s\n",
      "  âœ“ Extracted 1968 events\n",
      "    Significant: 804\n",
      "    Stationary: 1164\n",
      "âœ“ Test predictions: survival_analysis_results_Baltic_Data/test_predicted_events.csv\n",
      "âœ“ Validation ground truth: survival_analysis_results_Baltic_Data/validation_ground_truth_events.csv\n",
      "âœ“ Test ground truth: survival_analysis_results_Baltic_Data/test_ground_truth_events.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. SAVE PREDICTIONS (CSV)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“ Saving predictions...\")\n",
    "\n",
    "# Generate predictions for all splits using best model\n",
    "best_predictor = best_results['predictor']\n",
    "\n",
    "# Validation predictions\n",
    "val_predicted = best_predictor.predict(\n",
    "    val_features,\n",
    "    apply_rba_postprocess=True,\n",
    "    min_event_duration=3,\n",
    "    min_gap=2\n",
    ")\n",
    "val_pred_filename = os.path.join(OUTPUT_DIR, 'validation_predicted_events.csv')\n",
    "val_predicted.to_csv(val_pred_filename, index=False)\n",
    "print(f\"âœ“ Validation predictions: {val_pred_filename}\")\n",
    "\n",
    "# Test predictions\n",
    "test_predicted = best_predictor.predict(\n",
    "    test_features,\n",
    "    apply_rba_postprocess=True,\n",
    "    min_event_duration=3,\n",
    "    min_gap=2\n",
    ")\n",
    "test_pred_filename = os.path.join(OUTPUT_DIR, 'test_predicted_events.csv')\n",
    "test_predicted.to_csv(test_pred_filename, index=False)\n",
    "print(f\"âœ“ Test predictions: {test_pred_filename}\")\n",
    "\n",
    "# Ground truth events\n",
    "val_gt_filename = os.path.join(OUTPUT_DIR, 'validation_ground_truth_events.csv')\n",
    "val_events.to_csv(val_gt_filename, index=False)\n",
    "print(f\"âœ“ Validation ground truth: {val_gt_filename}\")\n",
    "\n",
    "test_gt_filename = os.path.join(OUTPUT_DIR, 'test_ground_truth_events.csv')\n",
    "test_events.to_csv(test_gt_filename, index=False)\n",
    "print(f\"âœ“ Test ground truth: {test_gt_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647ec95a-435d-47a1-a869-1f32693245a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Saving comparison table...\n",
      "âœ“ Comparison table: survival_analysis_results_Baltic_Data/configuration_comparison.csv\n",
      "\n",
      "ğŸ” Saving feature importance...\n",
      "âœ“ Feature importance: survival_analysis_results_Baltic_Data/feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. SAVE COMPARISON TABLE (CSV)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Saving comparison table...\")\n",
    "\n",
    "comparison_data = []\n",
    "for config, results in results_direct.items():\n",
    "    comparison_data.append({\n",
    "        'Configuration': config.upper(),\n",
    "        'Features': len(results['predictor'].feature_columns),\n",
    "        'Val_Precision': results['validation']['overall']['precision'],\n",
    "        'Val_Recall': results['validation']['overall']['recall'],\n",
    "        'Val_F1': results['validation']['overall']['f1_score'],\n",
    "        'Test_Precision': results['test']['overall']['precision'],\n",
    "        'Test_Recall': results['test']['overall']['recall'],\n",
    "        'Test_F1': results['test']['overall']['f1_score'],\n",
    "        'Training_Time_Min': results.get('training_time', 0) / 60,\n",
    "        'Prediction_Time_Sec': results.get('prediction_time', 0),\n",
    "        'Total_Time_Min': results.get('total_time', 0) / 60\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_filename = os.path.join(OUTPUT_DIR, 'configuration_comparison.csv')\n",
    "comparison_df.to_csv(comparison_filename, index=False)\n",
    "print(f\"âœ“ Comparison table: {comparison_filename}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SAVE FEATURE IMPORTANCE (CSV)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ” Saving feature importance...\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': best_results['predictor'].feature_columns,\n",
    "    'importance': best_results['predictor'].event_detector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_imp_filename = os.path.join(OUTPUT_DIR, 'feature_importance.csv')\n",
    "feature_importance.to_csv(feature_imp_filename, index=False)\n",
    "print(f\"âœ“ Feature importance: {feature_imp_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8803d24-3492-455f-aab2-a2e8dab064e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Creating README...\n",
      "âœ“ README: survival_analysis_results_Baltic_Data/README.md\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. CREATE README\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“„ Creating README...\")\n",
    "\n",
    "readme_content = f\"\"\"# Direct Event Prediction Results\n",
    "\n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Model:** Direct Event Prediction using Random Forest\n",
    "**Dataset:** Baltic Eagle Wind Turbine\n",
    "\n",
    "## Best Configuration: {best_name.upper()}\n",
    "\n",
    "### Performance Metrics\n",
    "- **Test F1-Score:** {best_results['test']['overall']['f1_score']:.4f} ({best_results['test']['overall']['f1_score']*100:.1f}%)\n",
    "- **Test Precision:** {best_results['test']['overall']['precision']:.4f}\n",
    "- **Test Recall:** {best_results['test']['overall']['recall']:.4f}\n",
    "\n",
    "### Timing\n",
    "- **Training Time:** {best_results.get('training_time', 0)/60:.1f} minutes\n",
    "- **Prediction Time:** {best_results.get('prediction_time', 0):.2f} seconds\n",
    "- **Throughput:** {len(test_features) / best_results.get('prediction_time', 1):.0f} samples/second\n",
    "\n",
    "### Model Details\n",
    "- **Features Used:** {len(best_results['predictor'].feature_columns)}\n",
    "- **Event Detector:** Random Forest ({best_results['predictor'].event_detector.n_estimators} trees, max_depth={best_results['predictor'].event_detector.max_depth})\n",
    "- **Event Type Classifier:** Random Forest ({best_results['predictor'].event_type_classifier.n_estimators if best_results['predictor'].event_type_classifier else 'N/A'} trees)\n",
    "\n",
    "## Files in this Directory\n",
    "\n",
    "1. **best_model_{best_name}.pkl** - Trained model (pickle format)\n",
    "2. **model_architecture.json** - Model architecture and hyperparameters\n",
    "3. **complete_results.json** - All performance metrics and metadata\n",
    "4. **validation_predicted_events.csv** - Predicted events for validation set\n",
    "5. **test_predicted_events.csv** - Predicted events for test set\n",
    "6. **validation_ground_truth_events.csv** - Ground truth events (validation)\n",
    "7. **test_ground_truth_events.csv** - Ground truth events (test)\n",
    "8. **configuration_comparison.csv** - Comparison of all configurations\n",
    "9. **feature_importance.csv** - Feature importance rankings\n",
    "10. **README.md** - This file\n",
    "\n",
    "## How to Load the Model\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "with open('best_model_{best_name}.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(your_features)\n",
    "```\n",
    "\n",
    "## Comparison to Previous Approaches\n",
    "\n",
    "| Approach | F1-Score | Method |\n",
    "|----------|----------|--------|\n",
    "| SARIMAX (indirect) | 91.47% | Time series â†’ RBA extraction |\n",
    "| Survival (6 features) | 41.29% | Inter-event timing |\n",
    "| **Direct RF (this)** | **{best_results['test']['overall']['f1_score']*100:.1f}%** | **Point-wise classification** |\n",
    "\n",
    "## Event Detection Quality: {quality}\n",
    "\n",
    "The model successfully predicts events directly from raw features with high accuracy and real-time performance.\n",
    "\"\"\"\n",
    "\n",
    "readme_filename = os.path.join(OUTPUT_DIR, 'README.md')\n",
    "with open(readme_filename, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "print(f\"âœ“ README: {readme_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d532ca6e-fd6d-448b-abb6-62fb5cd1855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING COMPREHENSIVE VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Creating model comparison dashboard...\n",
      "âœ“ Saved: survival_analysis_results_Baltic_Data/visualizations/01_model_comparison_dashboard.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMPREHENSIVE VISUALIZATIONS\n",
    "1. Model comparison (MINIMAL vs BALANCED vs FULL)\n",
    "2. Actual vs Predicted events on time series\n",
    "3. Performance breakdown by event type\n",
    "4. Confusion matrices and error analysis\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory for plots\n",
    "PLOT_DIR = os.path.join(OUTPUT_DIR, \"visualizations\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 1: MODEL COMPARISON DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š Creating model comparison dashboard...\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Extract data for all configurations\n",
    "configs = ['minimal', 'balanced', 'full']\n",
    "config_names = [c.upper() for c in configs]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "# Metrics\n",
    "test_f1 = [results_direct[c]['test']['overall']['f1_score'] for c in configs]\n",
    "test_precision = [results_direct[c]['test']['overall']['precision'] for c in configs]\n",
    "test_recall = [results_direct[c]['test']['overall']['recall'] for c in configs]\n",
    "val_f1 = [results_direct[c]['validation']['overall']['f1_score'] for c in configs]\n",
    "train_times = [results_direct[c].get('training_time', 0) / 60 for c in configs]\n",
    "feature_counts = [len(results_direct[c]['predictor'].feature_columns) for c in configs]\n",
    "\n",
    "# Plot 1: F1-Score Comparison (Test vs Validation)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "x = np.arange(len(configs))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, val_f1, width, label='Validation', alpha=0.8, color=colors)\n",
    "ax1.bar(x + width/2, test_f1, width, label='Test', alpha=0.8, color=colors, hatch='//')\n",
    "ax1.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('F1-Score: Validation vs Test', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(config_names)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1])\n",
    "# Add value labels\n",
    "for i, (v, t) in enumerate(zip(val_f1, test_f1)):\n",
    "    ax1.text(i - width/2, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax1.text(i + width/2, t + 0.02, f'{t:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Precision-Recall Tradeoff\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for i, config in enumerate(configs):\n",
    "    ax2.scatter(test_precision[i], test_recall[i], s=300, alpha=0.7, \n",
    "               color=colors[i], label=config_names[i], edgecolors='black', linewidth=2)\n",
    "    ax2.annotate(config_names[i], (test_precision[i], test_recall[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Perfect Balance')\n",
    "ax2.set_xlabel('Precision', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Recall', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Precision-Recall Tradeoff', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0.5, 0.85])\n",
    "ax2.set_ylim([0.5, 0.85])\n",
    "\n",
    "# Plot 3: Training Time vs Performance\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "scatter = ax3.scatter(train_times, test_f1, s=[f*3 for f in feature_counts], \n",
    "                     alpha=0.7, c=colors, edgecolors='black', linewidth=2)\n",
    "for i, config in enumerate(configs):\n",
    "    ax3.annotate(f'{config_names[i]}\\n({feature_counts[i]} feat)', \n",
    "                (train_times[i], test_f1[i]), \n",
    "                xytext=(10, -5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "ax3.set_xlabel('Training Time (minutes)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Test F1-Score', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Efficiency: Time vs Performance', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature Count Impact\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(feature_counts, test_f1, marker='o', linewidth=2, markersize=10, \n",
    "        color='#2ecc71', label='Test F1')\n",
    "ax4.plot(feature_counts, val_f1, marker='s', linewidth=2, markersize=10, \n",
    "        color='#3498db', label='Val F1', linestyle='--')\n",
    "ax4.set_xlabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Feature Count vs Performance', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "for i, (f, t) in enumerate(zip(feature_counts, test_f1)):\n",
    "    ax4.annotate(config_names[i], (f, t), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Plot 5: Event Type Performance (Stacked Bar)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "sig_f1 = [results_direct[c]['test']['significant']['f1_score'] for c in configs]\n",
    "stat_f1 = [results_direct[c]['test']['stationary']['f1_score'] for c in configs]\n",
    "x = np.arange(len(configs))\n",
    "width = 0.4\n",
    "ax5.bar(x, sig_f1, width, label='Significant Events', alpha=0.8, color='#e74c3c')\n",
    "ax5.bar(x, stat_f1, width, bottom=sig_f1, label='Stationary Events', alpha=0.8, color='#f39c12')\n",
    "ax5.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Performance by Event Type', fontsize=12, fontweight='bold')\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(config_names)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Confusion Matrix - Best Model\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "best_metrics = best_results['test']['overall']\n",
    "tp = best_metrics['true_positives']\n",
    "fp = best_metrics['false_positives']\n",
    "fn = best_metrics['false_negatives']\n",
    "tn = 0  # Not directly available in our metrics\n",
    "\n",
    "confusion_data = np.array([[tp, fn], [fp, 0]])\n",
    "im = ax6.imshow(confusion_data[:, :1], cmap='YlGn', aspect='auto')\n",
    "ax6.set_xticks([0])\n",
    "ax6.set_yticks([0, 1])\n",
    "ax6.set_xticklabels(['Positive'])\n",
    "ax6.set_yticklabels(['True Event', 'False Alarm'])\n",
    "ax6.set_title(f'Event Detection - {best_name.upper()}', fontsize=12, fontweight='bold')\n",
    "# Add text annotations\n",
    "ax6.text(0, 0, f'TP\\n{tp}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax6.text(0, 1, f'FP\\n{fp}', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax6.set_ylabel('Prediction', fontsize=11, fontweight='bold')\n",
    "ax6.set_xlabel(f'Recall: {best_metrics[\"recall\"]:.3f}\\nPrecision: {best_metrics[\"precision\"]:.3f}', \n",
    "              fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 7: Training Time Breakdown\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "train_time_data = [train_times[i] for i in range(len(configs))]\n",
    "bars = ax7.barh(config_names, train_time_data, color=colors, alpha=0.8)\n",
    "ax7.set_xlabel('Training Time (minutes)', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Training Time Comparison', fontsize=12, fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3, axis='x')\n",
    "for i, (bar, time) in enumerate(zip(bars, train_time_data)):\n",
    "    ax7.text(time + 2, i, f'{time:.1f} min', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 8: Performance Radar Chart\n",
    "ax8 = fig.add_subplot(gs[2, 1], projection='polar')\n",
    "categories = ['F1-Score', 'Precision', 'Recall', 'Speed\\n(inv. time)', 'Efficiency\\n(F1/features)']\n",
    "N = len(categories)\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    values = [\n",
    "        test_f1[i],\n",
    "        test_precision[i],\n",
    "        test_recall[i],\n",
    "        1 / (train_times[i] / 10),  # Normalized inverse time\n",
    "        test_f1[i] / (feature_counts[i] / 100)  # Normalized efficiency\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax8.plot(angles, values, 'o-', linewidth=2, label=config_names[i], color=colors[i])\n",
    "    ax8.fill(angles, values, alpha=0.15, color=colors[i])\n",
    "\n",
    "ax8.set_xticks(angles[:-1])\n",
    "ax8.set_xticklabels(categories, fontsize=9)\n",
    "ax8.set_ylim(0, 1)\n",
    "ax8.set_title('Multi-Metric Comparison', fontsize=12, fontweight='bold', pad=20)\n",
    "ax8.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax8.grid(True)\n",
    "\n",
    "# Plot 9: Summary Statistics Table\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "\n",
    "summary_data = []\n",
    "for i, config in enumerate(configs):\n",
    "    summary_data.append([\n",
    "        config_names[i],\n",
    "        f\"{test_f1[i]:.3f}\",\n",
    "        f\"{test_precision[i]:.3f}\",\n",
    "        f\"{test_recall[i]:.3f}\",\n",
    "        f\"{feature_counts[i]}\",\n",
    "        f\"{train_times[i]:.1f}m\"\n",
    "    ])\n",
    "\n",
    "table = ax9.table(cellText=summary_data,\n",
    "                 colLabels=['Config', 'F1', 'Prec', 'Rec', 'Feat', 'Time'],\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style table header\n",
    "for i in range(6):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Highlight best values\n",
    "best_idx = test_f1.index(max(test_f1))\n",
    "for i in range(6):\n",
    "    table[(best_idx + 1, i)].set_facecolor('#2ecc71')\n",
    "    table[(best_idx + 1, i)].set_alpha(0.3)\n",
    "\n",
    "ax9.set_title('Performance Summary', fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "plt.suptitle('Direct Event Prediction - Model Comparison Dashboard', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "dashboard_file = os.path.join(PLOT_DIR, '01_model_comparison_dashboard.png')\n",
    "plt.savefig(dashboard_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: {dashboard_file}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf5d50dd-b47c-4a57-b96e-2b9fcbe3077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Creating time series comparison...\n",
      "  Generating predictions for visualization...\n",
      "    MINIMAL: 2849 events\n",
      "    BALANCED: 2204 events\n",
      "    FULL: 2140 events\n",
      "âœ“ Saved: survival_analysis_results_Baltic_Data/visualizations/02_timeseries_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 2: TIME SERIES WITH ACTUAL VS PREDICTED EVENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“ˆ Creating time series comparison...\")\n",
    "\n",
    "# Select a window from test set for visualization\n",
    "window_size = 2000  # Adjust based on your needs\n",
    "window_start = 0\n",
    "window_end = min(window_size, len(test_raw))\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(20, 16), sharex=True)\n",
    "\n",
    "# Get predictions for all models\n",
    "print(\"  Generating predictions for visualization...\")\n",
    "predictions = {}\n",
    "for config in configs:\n",
    "    predictor = results_direct[config]['predictor']\n",
    "    \n",
    "    # Temporarily store prediction method\n",
    "    pred_method = predictor.predict\n",
    "    \n",
    "    # Get predictions without timing (to avoid conflicts)\n",
    "    pred_events = []\n",
    "    \n",
    "    # Manual prediction without internal timing\n",
    "    X = test_features[predictor.feature_columns].fillna(0).values\n",
    "    X_scaled = predictor.scaler.transform(X)\n",
    "    \n",
    "    # Predict event probabilities\n",
    "    event_probs = predictor.event_detector.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Predict event types\n",
    "    if predictor.event_type_classifier is not None:\n",
    "        type_probs = predictor.event_type_classifier.predict_proba(X_scaled)[:, 1]\n",
    "    else:\n",
    "        type_probs = np.random.rand(len(X))\n",
    "    \n",
    "    # Convert to events\n",
    "    event_threshold = 0.5\n",
    "    is_event = (event_probs > event_threshold).astype(int)\n",
    "    \n",
    "    # Extract event segments\n",
    "    in_event = False\n",
    "    event_start = 0\n",
    "    event_types_segment = []\n",
    "    \n",
    "    for t in range(len(is_event)):\n",
    "        if is_event[t] == 1 and not in_event:\n",
    "            in_event = True\n",
    "            event_start = t\n",
    "            event_types_segment = [type_probs[t]]\n",
    "        elif is_event[t] == 1 and in_event:\n",
    "            event_types_segment.append(type_probs[t])\n",
    "        elif is_event[t] == 0 and in_event:\n",
    "            event_end = t - 1\n",
    "            duration = event_end - event_start + 1\n",
    "            \n",
    "            if duration >= 3:  # min_event_duration\n",
    "                avg_type_prob = np.mean(event_types_segment)\n",
    "                event_type = 'significant' if avg_type_prob > 0.5 else 'stationary'\n",
    "                \n",
    "                pred_events.append({\n",
    "                    't1': event_start,\n",
    "                    't2': event_end,\n",
    "                    'event_type': event_type,\n",
    "                    'turbine_id': 1\n",
    "                })\n",
    "            \n",
    "            in_event = False\n",
    "    \n",
    "    # Handle event at end\n",
    "    if in_event:\n",
    "        event_end = len(is_event) - 1\n",
    "        duration = event_end - event_start + 1\n",
    "        if duration >= 3:\n",
    "            pred_events.append({\n",
    "                't1': event_start,\n",
    "                't2': event_end,\n",
    "                'event_type': 'significant' if np.mean(event_types_segment) > 0.5 else 'stationary',\n",
    "                'turbine_id': 1\n",
    "            })\n",
    "    \n",
    "    predictions[config] = pd.DataFrame(pred_events)\n",
    "    print(f\"    {config.upper()}: {len(predictions[config])} events\")\n",
    "\n",
    "# Time axis\n",
    "time_idx = np.arange(window_start, window_end)\n",
    "\n",
    "# Plot 1: Ground Truth\n",
    "ax = axes[0]\n",
    "ax.plot(time_idx, test_raw['Turbine_1'].iloc[window_start:window_end], \n",
    "       'b-', linewidth=1, alpha=0.7, label='Power Output')\n",
    "\n",
    "# Overlay ground truth events\n",
    "for _, event in test_events.iterrows():\n",
    "    if window_start <= event['t1'] < window_end:\n",
    "        color = '#e74c3c' if event['event_type'] == 'significant' else '#f39c12'\n",
    "        alpha = 0.3\n",
    "        ax.axvspan(event['t1'], min(event['t2'], window_end), \n",
    "                  alpha=alpha, color=color, linewidth=0)\n",
    "\n",
    "ax.set_ylabel('Power (kW)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Ground Truth Events (RBA-Extracted)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add legend for event types\n",
    "sig_patch = mpatches.Patch(color='#e74c3c', alpha=0.3, label='Significant Event')\n",
    "stat_patch = mpatches.Patch(color='#f39c12', alpha=0.3, label='Stationary Event')\n",
    "ax.legend(handles=[sig_patch, stat_patch], loc='upper left', fontsize=10)\n",
    "\n",
    "# Plot 2-4: Predictions from each model\n",
    "for idx, (config, color) in enumerate(zip(configs, colors), 1):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(time_idx, test_raw['Turbine_1'].iloc[window_start:window_end], \n",
    "           'gray', linewidth=1, alpha=0.5, label='Power Output')\n",
    "    \n",
    "    # Overlay predicted events\n",
    "    pred_events = predictions[config]\n",
    "    for _, event in pred_events.iterrows():\n",
    "        if window_start <= event['t1'] < window_end:\n",
    "            event_color = '#e74c3c' if event['event_type'] == 'significant' else '#f39c12'\n",
    "            ax.axvspan(event['t1'], min(event['t2'], window_end), \n",
    "                      alpha=0.3, color=event_color, linewidth=0)\n",
    "            \n",
    "            # Add edge to distinguish predictions\n",
    "            ax.axvline(event['t1'], color=event_color, alpha=0.8, linewidth=2, linestyle='--')\n",
    "            ax.axvline(min(event['t2'], window_end), color=event_color, alpha=0.8, linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Add performance metrics to title\n",
    "    f1 = results_direct[config]['test']['overall']['f1_score']\n",
    "    prec = results_direct[config]['test']['overall']['precision']\n",
    "    rec = results_direct[config]['test']['overall']['recall']\n",
    "    \n",
    "    ax.set_ylabel('Power (kW)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{config.upper()} Predictions (F1={f1:.3f}, P={prec:.3f}, R={rec:.3f})', \n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(handles=[sig_patch, stat_patch], loc='upper left', fontsize=10)\n",
    "\n",
    "axes[-1].set_xlabel('Time Step', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Actual vs Predicted Events - Test Set (First {window_size} samples)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "timeseries_file = os.path.join(PLOT_DIR, '02_timeseries_comparison.png')\n",
    "plt.savefig(timeseries_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: {timeseries_file}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52463c0-8ef0-4465-bf23-1f950dbae677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Creating event matching analysis...\n",
      "âœ“ Saved: survival_analysis_results_Baltic_Data/visualizations/03_event_matching_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 3: DETAILED EVENT MATCHING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ” Creating event matching analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, (config, color) in enumerate(zip(configs, colors)):\n",
    "    # Get metrics\n",
    "    metrics = results_direct[config]['test']\n",
    "    \n",
    "    # Plot for overall\n",
    "    ax = axes[0, idx]\n",
    "    categories = ['True\\nPositives', 'False\\nPositives', 'False\\nNegatives']\n",
    "    values = [\n",
    "        metrics['overall']['true_positives'],\n",
    "        metrics['overall']['false_positives'],\n",
    "        metrics['overall']['false_negatives']\n",
    "    ]\n",
    "    colors_bar = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "    bars = ax.bar(categories, values, color=colors_bar, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(val)}',\n",
    "               ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{config.upper()} - Overall', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot for event types\n",
    "    ax = axes[1, idx]\n",
    "    sig_tp = metrics['significant']['true_positives']\n",
    "    sig_fp = metrics['significant']['false_positives']\n",
    "    sig_fn = metrics['significant']['false_negatives']\n",
    "    stat_tp = metrics['stationary']['true_positives']\n",
    "    stat_fp = metrics['stationary']['false_positives']\n",
    "    stat_fn = metrics['stationary']['false_negatives']\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, [sig_tp, sig_fp, sig_fn], width, \n",
    "                   label='Significant', color='#e74c3c', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    bars2 = ax.bar(x + width/2, [stat_tp, stat_fp, stat_fn], width,\n",
    "                   label='Stationary', color='#f39c12', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f'{config.upper()} - By Event Type', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['TP', 'FP', 'FN'])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Event Detection Analysis - True Positives, False Positives, False Negatives', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "matching_file = os.path.join(PLOT_DIR, '03_event_matching_analysis.png')\n",
    "plt.savefig(matching_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: {matching_file}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abc5bc0b-7c03-4c7e-84b4-d9c38b4390a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Creating feature importance visualization...\n",
      "âœ“ Saved: survival_analysis_results_Baltic_Data/visualizations/04_feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 4: FEATURE IMPORTANCE - BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ¯ Creating feature importance visualization...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Get feature importance\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': best_results['predictor'].feature_columns,\n",
    "    'importance': best_results['predictor'].event_detector.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features\n",
    "top_features = feature_imp.head(20)\n",
    "\n",
    "# Plot 1: Bar chart\n",
    "colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "bars = ax1.barh(range(len(top_features)), top_features['importance'], color=colors_gradient, edgecolor='black', linewidth=1)\n",
    "ax1.set_yticks(range(len(top_features)))\n",
    "ax1.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "ax1.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Top 20 Features - {best_name.upper()} Model', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, imp) in enumerate(zip(bars, top_features['importance'])):\n",
    "    ax1.text(imp + 0.002, i, f'{imp:.4f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Cumulative importance\n",
    "cumsum = feature_imp['importance'].cumsum()\n",
    "ax2.plot(range(len(feature_imp)), cumsum, linewidth=3, color='#2ecc71', marker='o', markersize=4)\n",
    "ax2.axhline(y=0.8, color='r', linestyle='--', linewidth=2, label='80% threshold')\n",
    "ax2.axhline(y=0.9, color='orange', linestyle='--', linewidth=2, label='90% threshold')\n",
    "ax2.fill_between(range(len(feature_imp)), cumsum, alpha=0.3, color='#2ecc71')\n",
    "ax2.set_xlabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Cumulative Importance', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "# Find features needed for 80% and 90%\n",
    "n_80 = (cumsum >= 0.8).argmax() + 1\n",
    "n_90 = (cumsum >= 0.9).argmax() + 1\n",
    "ax2.axvline(x=n_80, color='r', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=n_90, color='orange', linestyle=':', alpha=0.5)\n",
    "ax2.text(n_80, 0.5, f'{n_80} features\\n(80%)', ha='center', fontsize=10, \n",
    "        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "ax2.text(n_90, 0.7, f'{n_90} features\\n(90%)', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "feature_imp_file = os.path.join(PLOT_DIR, '04_feature_importance.png')\n",
    "plt.savefig(feature_imp_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: {feature_imp_file}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59e8dbb0-5ebc-4bbd-9661-eba0f59c2880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Creating zoomed-in event comparison...\n",
      "âœ“ Saved: survival_analysis_results_Baltic_Data/visualizations/05_zoomed_event_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 5: ZOOMED-IN EVENT COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ”¬ Creating zoomed-in event comparison...\")\n",
    "\n",
    "# Find interesting events (ones where models disagree)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
    "\n",
    "# Select 6 random ground truth events\n",
    "sample_events = test_events.sample(min(6, len(test_events)), random_state=42)\n",
    "\n",
    "for idx, (_, gt_event) in enumerate(sample_events.iterrows()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Define window around event\n",
    "    event_start = int(gt_event['t1'])\n",
    "    event_end = int(gt_event['t2'])\n",
    "    margin = 50\n",
    "    win_start = max(0, event_start - margin)\n",
    "    win_end = min(len(test_raw), event_end + margin)\n",
    "    \n",
    "    # Plot power output\n",
    "    time_window = np.arange(win_start, win_end)\n",
    "    ax.plot(time_window, test_raw['Turbine_1'].iloc[win_start:win_end],\n",
    "           'b-', linewidth=2, label='Power', alpha=0.7)\n",
    "    \n",
    "    # Ground truth event\n",
    "    ax.axvspan(event_start, event_end, alpha=0.2, color='green', label='Ground Truth')\n",
    "    \n",
    "    # Predicted events from all models\n",
    "    colors_pred = ['red', 'orange', 'purple']\n",
    "    for config, color in zip(configs, colors_pred):\n",
    "        pred_events = predictions[config]\n",
    "        for _, pred in pred_events.iterrows():\n",
    "            if event_start - margin <= pred['t1'] <= event_end + margin:\n",
    "                ax.axvspan(pred['t1'], pred['t2'], alpha=0.15, color=color)\n",
    "                ax.axvline(pred['t1'], color=color, linewidth=1.5, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time Step', fontsize=10)\n",
    "    ax.set_ylabel('Power (kW)', fontsize=10)\n",
    "    ax.set_title(f'Event {idx+1}: {gt_event[\"event_type\"].capitalize()} '\n",
    "                f'(t={event_start}-{event_end})', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Custom legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='green', alpha=0.5, linewidth=10, label='Ground Truth'),\n",
    "        Line2D([0], [0], color='red', alpha=0.5, linewidth=10, label='MINIMAL'),\n",
    "        Line2D([0], [0], color='orange', alpha=0.5, linewidth=10, label='BALANCED'),\n",
    "        Line2D([0], [0], color='purple', alpha=0.5, linewidth=10, label='FULL')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "\n",
    "plt.suptitle('Zoomed-In Event Comparison - Ground Truth vs Predictions', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "zoomed_file = os.path.join(PLOT_DIR, '05_zoomed_event_comparison.png')\n",
    "plt.savefig(zoomed_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: {zoomed_file}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b22ef-edb2-42ec-bfa8-54db90624e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
